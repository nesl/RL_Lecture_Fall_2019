{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Developing a DQN algorithm from Scratch for Cartpole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/sandeep/miniconda3/envs/rlenv/bin/python'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See that, we are using the correct environment\n",
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Code modified from: https://github.com/keon/deep-q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "import keras.backend as k\n",
    "from collections import deque\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME = 'CartPole-v0'\n",
    "\n",
    "# Get the environment and extract the number of actions available in the Cartpole problem\n",
    "env = gym.make(ENV_NAME)\n",
    "np.random.seed(1)\n",
    "env.seed(1)\n",
    "nb_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "input_shape = env.observation_space.shape[0]\n",
    "print(input_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple NN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/sandeep/miniconda3/envs/rlenv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:71: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/sandeep/miniconda3/envs/rlenv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:514: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/sandeep/miniconda3/envs/rlenv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4076: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/sandeep/miniconda3/envs/rlenv/lib/python3.6/site-packages/keras/optimizers.py:757: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 16)                80        \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 34        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 114\n",
      "Trainable params: 114\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(16, input_dim=input_shape, activation='relu'))\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('linear'))\n",
    "model.compile(loss='mse',\n",
    "                      optimizer=Adam(lr=0.001))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Developing a DQN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Q-learning Agent\n",
    "class SimpleAgent:\n",
    "    def __init__(self, action_size, model,exp=1.0):\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=10000)\n",
    "        self.gamma = 0.99    # discount factor\n",
    "        \n",
    "        self.epsilon = exp  # exploration rate\n",
    "        \n",
    "        self.epsilon_min = 0.01 # We always do some exploration\n",
    "        \n",
    "        self.epsilon_decay = 0.999\n",
    "        \n",
    "        self.model = model\n",
    "        \n",
    "    def buffer(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        \n",
    "    def get_act(self, state):\n",
    "        #Return Random action\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        \n",
    "        act_values = self.model.predict(state)\n",
    "        return np.argmax(act_values[0])  # returns action\n",
    "    \n",
    "    #This is where training happens\n",
    "    def replay(self, batch_size):\n",
    "        \n",
    "        #Get a Random Minibatch\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        \n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "              target = reward + self.gamma * \\\n",
    "                       np.amax(self.model.predict(next_state)[0])\n",
    "            target_f = self.model.predict(state)\n",
    "            \n",
    "            #only update the target for action taken\n",
    "            target_f[0][action] = target\n",
    "            \n",
    "            \n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "        \n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize gym environment and the agent\n",
    "env = gym.make('CartPole-v0')\n",
    "nb_actions = env.action_space.n\n",
    "agent = SimpleAgent(nb_actions,model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/sandeep/miniconda3/envs/rlenv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:171: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/sandeep/miniconda3/envs/rlenv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:178: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.load_weights('Trained_Scratch_Cartpole.h5f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training of the Model: Let us Play!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_size = env.observation_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0/20, steps: 18, exploration: 1.0\n",
      "episode: 1/20, steps: 19, exploration: 1.0\n",
      "episode: 2/20, steps: 28, exploration: 1.0\n",
      "episode: 3/20, steps: 36, exploration: 0.96\n",
      "episode: 4/20, steps: 20, exploration: 0.94\n",
      "episode: 5/20, steps: 41, exploration: 0.9\n",
      "episode: 6/20, steps: 13, exploration: 0.89\n",
      "episode: 7/20, steps: 39, exploration: 0.86\n",
      "episode: 8/20, steps: 41, exploration: 0.82\n",
      "episode: 9/20, steps: 33, exploration: 0.8\n",
      "episode: 10/20, steps: 60, exploration: 0.75\n",
      "episode: 11/20, steps: 44, exploration: 0.72\n",
      "episode: 12/20, steps: 30, exploration: 0.7\n",
      "episode: 13/20, steps: 199, exploration: 0.57\n",
      "episode: 14/20, steps: 73, exploration: 0.53\n",
      "episode: 15/20, steps: 83, exploration: 0.49\n",
      "episode: 16/20, steps: 81, exploration: 0.45\n",
      "episode: 17/20, steps: 156, exploration: 0.39\n",
      "episode: 18/20, steps: 189, exploration: 0.32\n",
      "episode: 19/20, steps: 199, exploration: 0.26\n"
     ]
    }
   ],
   "source": [
    "done = False\n",
    "batch_size = 64\n",
    "num_episodes = 20\n",
    "max_steps = 200\n",
    "\n",
    "\n",
    "        \n",
    "for e in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, input_shape]) #Reshape for later NN Training\n",
    "    for steps in range(max_steps):\n",
    "        #Visualization of the env\n",
    "        #env.render()\n",
    "\n",
    "        #Get the action from the Agent\n",
    "        action = agent.get_act(state)\n",
    "        \n",
    "        #Take this action\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        \n",
    "        reward = reward if not done else -10\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        agent.buffer(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        if done:\n",
    "            print(\"episode: {}/{}, steps: {}, exploration: {:.2}\"\n",
    "                  .format(e, num_episodes, steps, agent.epsilon))\n",
    "            break\n",
    "        if len(agent.memory) > batch_size:\n",
    "            agent.replay(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(\"Trained_Scratch_Cartpole.h5f\", overwrite=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_episode: 0/5, steps: 199, exploration: 0.0\n",
      "test_episode: 1/5, steps: 199, exploration: 0.0\n",
      "test_episode: 2/5, steps: 199, exploration: 0.0\n",
      "test_episode: 3/5, steps: 199, exploration: 0.0\n",
      "test_episode: 4/5, steps: 199, exploration: 0.0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "test_episode= 5\n",
    "model.load_weights('Trained_Scratch_Cartpole.h5f')\n",
    "\n",
    "agent = SimpleAgent(nb_actions,model,exp=0.0)\n",
    "\n",
    "for e in range(test_episode):\n",
    "    state = env.reset()\n",
    "    \n",
    "    for steps in range(200):\n",
    "        \n",
    "        state = np.reshape(state, [1, input_shape]) #Reshape for later NN input\n",
    "        \n",
    "        #Visualization of the env\n",
    "        env.render()\n",
    "\n",
    "        #Get the action from the Agent\n",
    "        action = agent.get_act(state)\n",
    "        \n",
    "        #Take this action\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        if done:\n",
    "            print(\"test_episode: {}/{}, steps: {}, exploration: {:.2}\"\n",
    "                  .format(e, test_episode, steps, agent.epsilon))\n",
    "            break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

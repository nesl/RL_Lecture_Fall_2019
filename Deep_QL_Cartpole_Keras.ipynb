{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q Learning using Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/sandeep/anaconda3/envs/rlenv/bin/python'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See that, we are using the correct environment\n",
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "import keras.backend as k\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import EpsGreedyQPolicy, LinearAnnealedPolicy\n",
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gym env and actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME = 'CartPole-v0'\n",
    "\n",
    "# Get the environment and extract the number of actions available in the Cartpole problem\n",
    "env = gym.make(ENV_NAME)\n",
    "np.random.seed(1)\n",
    "env.seed(1)\n",
    "nb_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 4)\n"
     ]
    }
   ],
   "source": [
    "input_shape=(1,) + env.observation_space.shape\n",
    "print(input_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple NN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/sandeep/anaconda3/envs/rlenv/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:71: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/sandeep/anaconda3/envs/rlenv/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:514: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/sandeep/anaconda3/envs/rlenv/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:4076: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "Dense_1 (Dense)              (None, 16)                80        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 34        \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 114\n",
      "Trainable params: 114\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "model.add(Dense(16, name =\"Dense_1\"))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('linear'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/sandeep/anaconda3/envs/rlenv/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:171: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/sandeep/anaconda3/envs/rlenv/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:178: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/sandeep/anaconda3/envs/rlenv/lib/python3.5/site-packages/keras/optimizers.py:757: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps', value_max=1.00,value_min=.05, value_test=.05,\n",
    "nb_steps=10000)\n",
    "\n",
    "memory = SequentialMemory(limit=10000, window_length=1)\n",
    "\n",
    "dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=10, target_model_update=100, policy=policy)\n",
    "\n",
    "dqn.compile(Adam(lr=0.001), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 50000 steps ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sandeep/anaconda3/envs/rlenv/lib/python3.5/site-packages/rl/memory.py:39: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    16/50000: episode: 1, duration: 2.231s, episode steps: 16, steps per second: 7, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.084 [-0.788, 1.345], loss: 1.072774, mean_absolute_error: 0.919518, mean_q: 0.609775, mean_eps: 0.998765\n",
      "    34/50000: episode: 2, duration: 0.056s, episode steps: 18, steps per second: 323, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: -0.098 [-1.294, 0.559], loss: 0.756542, mean_absolute_error: 0.768471, mean_q: 0.512654, mean_eps: 0.997672\n",
      "    54/50000: episode: 3, duration: 0.070s, episode steps: 20, steps per second: 287, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: 0.115 [-0.614, 1.473], loss: 0.555532, mean_absolute_error: 0.702276, mean_q: 0.470321, mean_eps: 0.995867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sandeep/anaconda3/envs/rlenv/lib/python3.5/site-packages/rl/memory.py:39: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    66/50000: episode: 4, duration: 0.120s, episode steps: 12, steps per second: 100, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.093 [-0.843, 1.525], loss: 0.477063, mean_absolute_error: 0.713304, mean_q: 0.587330, mean_eps: 0.994347\n",
      "    81/50000: episode: 5, duration: 0.049s, episode steps: 15, steps per second: 306, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.076 [-2.068, 1.368], loss: 0.404321, mean_absolute_error: 0.721789, mean_q: 0.691998, mean_eps: 0.993065\n",
      "   110/50000: episode: 6, duration: 0.076s, episode steps: 29, steps per second: 382, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.552 [0.000, 1.000], mean observation: -0.023 [-1.430, 0.929], loss: 0.424261, mean_absolute_error: 0.753687, mean_q: 0.746574, mean_eps: 0.990975\n",
      "   125/50000: episode: 7, duration: 0.039s, episode steps: 15, steps per second: 384, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.111 [-1.539, 0.810], loss: 0.560024, mean_absolute_error: 0.872495, mean_q: 0.834693, mean_eps: 0.988885\n",
      "   143/50000: episode: 8, duration: 0.047s, episode steps: 18, steps per second: 387, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.123 [-1.173, 2.281], loss: 0.435378, mean_absolute_error: 0.863836, mean_q: 0.932529, mean_eps: 0.987318\n",
      "   157/50000: episode: 9, duration: 0.038s, episode steps: 14, steps per second: 373, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.643 [0.000, 1.000], mean observation: -0.109 [-1.515, 0.960], loss: 0.356819, mean_absolute_error: 0.863637, mean_q: 1.049620, mean_eps: 0.985797\n",
      "   211/50000: episode: 10, duration: 0.133s, episode steps: 54, steps per second: 405, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: 0.086 [-1.023, 1.817], loss: 0.327481, mean_absolute_error: 0.905202, mean_q: 1.210714, mean_eps: 0.982567\n",
      "   226/50000: episode: 11, duration: 0.039s, episode steps: 15, steps per second: 380, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: 0.070 [-0.989, 1.477], loss: 0.619684, mean_absolute_error: 1.240432, mean_q: 1.594718, mean_eps: 0.979290\n",
      "   239/50000: episode: 12, duration: 0.034s, episode steps: 13, steps per second: 383, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: -0.119 [-1.106, 0.550], loss: 0.484544, mean_absolute_error: 1.275408, mean_q: 1.790601, mean_eps: 0.977960\n",
      "   259/50000: episode: 13, duration: 0.056s, episode steps: 20, steps per second: 354, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.350 [0.000, 1.000], mean observation: 0.054 [-1.164, 1.917], loss: 0.332340, mean_absolute_error: 1.233709, mean_q: 1.908492, mean_eps: 0.976392\n",
      "   276/50000: episode: 14, duration: 0.047s, episode steps: 17, steps per second: 360, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.647 [0.000, 1.000], mean observation: -0.086 [-2.071, 1.200], loss: 0.262762, mean_absolute_error: 1.263466, mean_q: 2.108370, mean_eps: 0.974635\n",
      "   313/50000: episode: 15, duration: 0.231s, episode steps: 37, steps per second: 160, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: 0.103 [-0.475, 1.655], loss: 0.447258, mean_absolute_error: 1.393121, mean_q: 2.258397, mean_eps: 0.972070\n",
      "   328/50000: episode: 16, duration: 0.251s, episode steps: 15, steps per second: 60, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.125 [-0.748, 1.582], loss: 0.694359, mean_absolute_error: 1.718146, mean_q: 2.578589, mean_eps: 0.969600\n",
      "   365/50000: episode: 17, duration: 0.615s, episode steps: 37, steps per second: 60, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.541 [0.000, 1.000], mean observation: -0.063 [-1.529, 0.959], loss: 0.449034, mean_absolute_error: 1.758394, mean_q: 3.084430, mean_eps: 0.967130\n",
      "   382/50000: episode: 18, duration: 0.287s, episode steps: 17, steps per second: 59, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: -0.114 [-1.252, 0.556], loss: 0.380875, mean_absolute_error: 1.692414, mean_q: 3.214168, mean_eps: 0.964565\n",
      "   403/50000: episode: 19, duration: 0.345s, episode steps: 21, steps per second: 61, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: -0.106 [-1.140, 0.541], loss: 0.461503, mean_absolute_error: 1.768271, mean_q: 3.366044, mean_eps: 0.962760\n",
      "   420/50000: episode: 20, duration: 0.282s, episode steps: 17, steps per second: 60, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.353 [0.000, 1.000], mean observation: 0.090 [-1.015, 1.826], loss: 1.121246, mean_absolute_error: 2.296876, mean_q: 3.544415, mean_eps: 0.960955\n",
      "   443/50000: episode: 21, duration: 0.372s, episode steps: 23, steps per second: 62, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: -0.070 [-1.294, 0.648], loss: 0.849912, mean_absolute_error: 2.303684, mean_q: 3.926499, mean_eps: 0.959055\n",
      "   454/50000: episode: 22, duration: 0.039s, episode steps: 11, steps per second: 284, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.123 [-1.991, 1.131], loss: 0.622986, mean_absolute_error: 2.312774, mean_q: 4.199299, mean_eps: 0.957440\n",
      "   496/50000: episode: 23, duration: 0.125s, episode steps: 42, steps per second: 336, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: 0.076 [-0.594, 1.543], loss: 0.584226, mean_absolute_error: 2.294934, mean_q: 4.342091, mean_eps: 0.954922\n",
      "   544/50000: episode: 24, duration: 0.121s, episode steps: 48, steps per second: 395, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: 0.081 [-0.781, 1.621], loss: 0.994227, mean_absolute_error: 2.769116, mean_q: 4.817140, mean_eps: 0.950647\n",
      "   568/50000: episode: 25, duration: 0.061s, episode steps: 24, steps per second: 393, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.042 [-1.478, 0.956], loss: 0.930136, mean_absolute_error: 2.928304, mean_q: 5.562609, mean_eps: 0.947228\n",
      "   592/50000: episode: 26, duration: 0.062s, episode steps: 24, steps per second: 388, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.375 [0.000, 1.000], mean observation: 0.066 [-1.206, 2.111], loss: 0.579140, mean_absolute_error: 2.880746, mean_q: 5.667959, mean_eps: 0.944948\n",
      "   616/50000: episode: 27, duration: 0.062s, episode steps: 24, steps per second: 388, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.065 [-1.322, 0.794], loss: 1.375142, mean_absolute_error: 3.285163, mean_q: 5.789807, mean_eps: 0.942668\n",
      "   638/50000: episode: 28, duration: 0.058s, episode steps: 22, steps per second: 380, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.117 [-0.978, 0.612], loss: 1.118762, mean_absolute_error: 3.474766, mean_q: 6.253216, mean_eps: 0.940482\n",
      "   651/50000: episode: 29, duration: 0.033s, episode steps: 13, steps per second: 394, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.074 [-1.687, 1.031], loss: 0.975591, mean_absolute_error: 3.479349, mean_q: 6.571020, mean_eps: 0.938820\n",
      "   671/50000: episode: 30, duration: 0.050s, episode steps: 20, steps per second: 399, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.350 [0.000, 1.000], mean observation: 0.059 [-1.362, 2.145], loss: 1.039628, mean_absolute_error: 3.504652, mean_q: 6.761600, mean_eps: 0.937253\n",
      "   686/50000: episode: 31, duration: 0.038s, episode steps: 15, steps per second: 390, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: 0.098 [-0.960, 1.563], loss: 1.046556, mean_absolute_error: 3.576328, mean_q: 6.989804, mean_eps: 0.935590\n",
      "   695/50000: episode: 32, duration: 0.024s, episode steps: 9, steps per second: 371, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.154 [-1.382, 2.271], loss: 0.725804, mean_absolute_error: 3.547312, mean_q: 7.036726, mean_eps: 0.934450\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   716/50000: episode: 33, duration: 0.055s, episode steps: 21, steps per second: 382, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.039 [-1.172, 1.703], loss: 1.650535, mean_absolute_error: 3.977781, mean_q: 7.086475, mean_eps: 0.933025\n",
      "   729/50000: episode: 34, duration: 0.036s, episode steps: 13, steps per second: 359, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.081 [-1.384, 2.057], loss: 1.834634, mean_absolute_error: 4.220618, mean_q: 7.646779, mean_eps: 0.931410\n",
      "   751/50000: episode: 35, duration: 0.057s, episode steps: 22, steps per second: 385, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: -0.123 [-1.498, 0.586], loss: 1.313973, mean_absolute_error: 4.168688, mean_q: 7.990619, mean_eps: 0.929747\n",
      "   775/50000: episode: 36, duration: 0.073s, episode steps: 24, steps per second: 328, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.375 [0.000, 1.000], mean observation: 0.017 [-1.337, 1.829], loss: 1.587183, mean_absolute_error: 4.217976, mean_q: 8.164430, mean_eps: 0.927563\n",
      "   788/50000: episode: 37, duration: 0.034s, episode steps: 13, steps per second: 382, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.111 [-1.986, 1.178], loss: 2.015623, mean_absolute_error: 4.297850, mean_q: 8.334807, mean_eps: 0.925805\n",
      "   799/50000: episode: 38, duration: 0.033s, episode steps: 11, steps per second: 337, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.101 [-0.824, 1.401], loss: 1.898607, mean_absolute_error: 4.274763, mean_q: 8.185547, mean_eps: 0.924665\n",
      "   822/50000: episode: 39, duration: 0.060s, episode steps: 23, steps per second: 385, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.304 [0.000, 1.000], mean observation: 0.085 [-1.778, 2.910], loss: 2.041434, mean_absolute_error: 4.650869, mean_q: 8.409048, mean_eps: 0.923050\n",
      "   855/50000: episode: 40, duration: 0.081s, episode steps: 33, steps per second: 408, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: 0.026 [-2.858, 2.161], loss: 2.117955, mean_absolute_error: 4.785252, mean_q: 9.170798, mean_eps: 0.920390\n",
      "   889/50000: episode: 41, duration: 0.085s, episode steps: 34, steps per second: 400, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: -0.062 [-1.310, 0.632], loss: 1.710869, mean_absolute_error: 4.767145, mean_q: 9.359811, mean_eps: 0.917208\n",
      "   942/50000: episode: 42, duration: 0.130s, episode steps: 53, steps per second: 409, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.623 [0.000, 1.000], mean observation: 0.004 [-3.531, 2.495], loss: 2.302601, mean_absolute_error: 5.188003, mean_q: 9.658619, mean_eps: 0.913075\n",
      "   963/50000: episode: 43, duration: 0.053s, episode steps: 21, steps per second: 400, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.619 [0.000, 1.000], mean observation: -0.054 [-1.862, 1.155], loss: 2.088386, mean_absolute_error: 5.332075, mean_q: 10.379819, mean_eps: 0.909560\n",
      "  1001/50000: episode: 44, duration: 0.097s, episode steps: 38, steps per second: 393, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.421 [0.000, 1.000], mean observation: 0.024 [-1.374, 1.939], loss: 2.335296, mean_absolute_error: 5.339004, mean_q: 10.364033, mean_eps: 0.906758\n",
      "  1023/50000: episode: 45, duration: 0.055s, episode steps: 22, steps per second: 396, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.048 [-1.015, 1.429], loss: 3.565232, mean_absolute_error: 5.906430, mean_q: 10.643154, mean_eps: 0.903907\n",
      "  1042/50000: episode: 46, duration: 0.047s, episode steps: 19, steps per second: 401, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.579 [0.000, 1.000], mean observation: -0.095 [-1.578, 0.981], loss: 2.556772, mean_absolute_error: 5.859035, mean_q: 11.174417, mean_eps: 0.901960\n",
      "  1060/50000: episode: 47, duration: 0.045s, episode steps: 18, steps per second: 400, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.611 [0.000, 1.000], mean observation: -0.091 [-1.708, 0.966], loss: 2.971019, mean_absolute_error: 5.896590, mean_q: 11.418047, mean_eps: 0.900202\n",
      "  1073/50000: episode: 48, duration: 0.033s, episode steps: 13, steps per second: 397, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.231 [0.000, 1.000], mean observation: 0.096 [-1.341, 2.191], loss: 3.116675, mean_absolute_error: 5.942059, mean_q: 11.332671, mean_eps: 0.898730\n",
      "  1085/50000: episode: 49, duration: 0.032s, episode steps: 12, steps per second: 373, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.118 [-2.025, 1.157], loss: 2.510951, mean_absolute_error: 5.802842, mean_q: 11.207940, mean_eps: 0.897542\n",
      "  1110/50000: episode: 50, duration: 0.062s, episode steps: 25, steps per second: 401, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.360 [0.000, 1.000], mean observation: 0.056 [-1.384, 2.261], loss: 2.781889, mean_absolute_error: 6.034573, mean_q: 11.376478, mean_eps: 0.895785\n",
      "  1157/50000: episode: 51, duration: 0.115s, episode steps: 47, steps per second: 410, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.447 [0.000, 1.000], mean observation: -0.014 [-1.378, 1.771], loss: 2.747228, mean_absolute_error: 6.327723, mean_q: 12.138609, mean_eps: 0.892365\n",
      "  1170/50000: episode: 52, duration: 0.036s, episode steps: 13, steps per second: 357, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.092 [-0.956, 1.439], loss: 2.830010, mean_absolute_error: 6.377700, mean_q: 12.498511, mean_eps: 0.889515\n",
      "  1186/50000: episode: 53, duration: 0.040s, episode steps: 16, steps per second: 395, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.312 [0.000, 1.000], mean observation: 0.100 [-1.148, 2.002], loss: 3.091374, mean_absolute_error: 6.347419, mean_q: 12.179946, mean_eps: 0.888138\n",
      "  1212/50000: episode: 54, duration: 0.066s, episode steps: 26, steps per second: 394, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: 0.059 [-0.582, 1.278], loss: 3.305525, mean_absolute_error: 6.579668, mean_q: 12.332291, mean_eps: 0.886143\n",
      "  1223/50000: episode: 55, duration: 0.028s, episode steps: 11, steps per second: 389, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.133 [-1.738, 2.774], loss: 2.577389, mean_absolute_error: 6.699484, mean_q: 12.619611, mean_eps: 0.884385\n",
      "  1240/50000: episode: 56, duration: 0.043s, episode steps: 17, steps per second: 393, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.647 [0.000, 1.000], mean observation: -0.097 [-1.852, 0.989], loss: 3.091596, mean_absolute_error: 6.775749, mean_q: 12.941991, mean_eps: 0.883055\n",
      "  1281/50000: episode: 57, duration: 0.100s, episode steps: 41, steps per second: 408, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.439 [0.000, 1.000], mean observation: 0.019 [-1.221, 2.010], loss: 2.994558, mean_absolute_error: 6.761186, mean_q: 13.029048, mean_eps: 0.880300\n",
      "  1310/50000: episode: 58, duration: 0.073s, episode steps: 29, steps per second: 399, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.448 [0.000, 1.000], mean observation: 0.086 [-0.659, 1.629], loss: 4.348561, mean_absolute_error: 6.961627, mean_q: 13.082908, mean_eps: 0.876975\n",
      "  1326/50000: episode: 59, duration: 0.040s, episode steps: 16, steps per second: 400, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.562 [0.000, 1.000], mean observation: -0.095 [-1.208, 0.636], loss: 4.562012, mean_absolute_error: 7.186365, mean_q: 13.192706, mean_eps: 0.874838\n",
      "  1338/50000: episode: 60, duration: 0.031s, episode steps: 12, steps per second: 384, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.115 [-1.188, 2.091], loss: 4.094825, mean_absolute_error: 7.167310, mean_q: 13.448103, mean_eps: 0.873507\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1369/50000: episode: 61, duration: 0.077s, episode steps: 31, steps per second: 401, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.419 [0.000, 1.000], mean observation: 0.049 [-1.147, 1.876], loss: 3.744569, mean_absolute_error: 7.154407, mean_q: 13.536773, mean_eps: 0.871465\n",
      "  1382/50000: episode: 62, duration: 0.034s, episode steps: 13, steps per second: 388, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.125 [-1.324, 2.166], loss: 4.025679, mean_absolute_error: 7.220718, mean_q: 13.802047, mean_eps: 0.869375\n",
      "  1403/50000: episode: 63, duration: 0.054s, episode steps: 21, steps per second: 388, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.081 [-0.798, 1.470], loss: 2.750321, mean_absolute_error: 7.162532, mean_q: 13.826633, mean_eps: 0.867760\n",
      "  1425/50000: episode: 64, duration: 0.055s, episode steps: 22, steps per second: 402, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.682 [0.000, 1.000], mean observation: -0.055 [-2.658, 1.725], loss: 4.657865, mean_absolute_error: 7.587726, mean_q: 14.077382, mean_eps: 0.865718\n",
      "  1436/50000: episode: 65, duration: 0.028s, episode steps: 11, steps per second: 391, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: 0.106 [-1.201, 1.819], loss: 3.642174, mean_absolute_error: 7.532018, mean_q: 14.202103, mean_eps: 0.864150\n",
      "  1451/50000: episode: 66, duration: 0.039s, episode steps: 15, steps per second: 383, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.093 [-1.020, 1.880], loss: 3.308502, mean_absolute_error: 7.574368, mean_q: 14.426186, mean_eps: 0.862915\n",
      "  1517/50000: episode: 67, duration: 0.159s, episode steps: 66, steps per second: 416, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.606 [0.000, 1.000], mean observation: 0.119 [-2.690, 2.661], loss: 3.660741, mean_absolute_error: 7.679900, mean_q: 14.512415, mean_eps: 0.859067\n",
      "  1528/50000: episode: 68, duration: 0.028s, episode steps: 11, steps per second: 394, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.141 [-1.348, 2.273], loss: 3.357366, mean_absolute_error: 7.864114, mean_q: 14.842618, mean_eps: 0.855410\n",
      "  1543/50000: episode: 69, duration: 0.040s, episode steps: 15, steps per second: 375, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.267 [0.000, 1.000], mean observation: 0.074 [-1.343, 2.171], loss: 3.498774, mean_absolute_error: 7.889198, mean_q: 15.012874, mean_eps: 0.854175\n",
      "  1556/50000: episode: 70, duration: 0.033s, episode steps: 13, steps per second: 397, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.129 [-0.608, 1.354], loss: 4.519582, mean_absolute_error: 7.975983, mean_q: 15.091250, mean_eps: 0.852845\n",
      "  1576/50000: episode: 71, duration: 0.050s, episode steps: 20, steps per second: 397, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: 0.102 [-0.599, 1.429], loss: 5.537451, mean_absolute_error: 8.024876, mean_q: 15.035879, mean_eps: 0.851277\n",
      "  1592/50000: episode: 72, duration: 0.040s, episode steps: 16, steps per second: 402, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.625 [0.000, 1.000], mean observation: -0.098 [-1.736, 0.943], loss: 4.396424, mean_absolute_error: 7.979936, mean_q: 14.960579, mean_eps: 0.849568\n",
      "  1612/50000: episode: 73, duration: 0.051s, episode steps: 20, steps per second: 390, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.650 [0.000, 1.000], mean observation: -0.036 [-1.926, 1.215], loss: 4.758879, mean_absolute_error: 8.046361, mean_q: 14.923660, mean_eps: 0.847857\n",
      "  1630/50000: episode: 74, duration: 0.046s, episode steps: 18, steps per second: 388, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.389 [0.000, 1.000], mean observation: 0.070 [-0.937, 1.663], loss: 5.439020, mean_absolute_error: 8.173750, mean_q: 14.953108, mean_eps: 0.846053\n",
      "  1642/50000: episode: 75, duration: 0.033s, episode steps: 12, steps per second: 369, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.833 [0.000, 1.000], mean observation: -0.126 [-2.562, 1.546], loss: 3.704275, mean_absolute_error: 8.118907, mean_q: 15.120791, mean_eps: 0.844628\n",
      "  1658/50000: episode: 76, duration: 0.046s, episode steps: 16, steps per second: 350, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.082 [-1.015, 1.485], loss: 4.448937, mean_absolute_error: 8.153966, mean_q: 15.184885, mean_eps: 0.843298\n",
      "  1677/50000: episode: 77, duration: 0.048s, episode steps: 19, steps per second: 400, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.316 [0.000, 1.000], mean observation: 0.047 [-1.391, 2.250], loss: 3.949386, mean_absolute_error: 8.080476, mean_q: 15.232981, mean_eps: 0.841635\n",
      "  1694/50000: episode: 78, duration: 0.042s, episode steps: 17, steps per second: 400, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.353 [0.000, 1.000], mean observation: 0.077 [-1.015, 1.733], loss: 3.474352, mean_absolute_error: 8.040190, mean_q: 15.346573, mean_eps: 0.839925\n",
      "  1740/50000: episode: 79, duration: 0.115s, episode steps: 46, steps per second: 400, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.120 [-1.413, 0.559], loss: 4.108441, mean_absolute_error: 8.304171, mean_q: 15.563812, mean_eps: 0.836932\n",
      "  1756/50000: episode: 80, duration: 0.040s, episode steps: 16, steps per second: 396, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.375 [0.000, 1.000], mean observation: 0.088 [-1.013, 1.762], loss: 3.688466, mean_absolute_error: 8.312236, mean_q: 15.750144, mean_eps: 0.833987\n",
      "  1771/50000: episode: 81, duration: 0.039s, episode steps: 15, steps per second: 389, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.110 [-0.969, 1.476], loss: 5.026151, mean_absolute_error: 8.436120, mean_q: 15.760142, mean_eps: 0.832515\n",
      "  1796/50000: episode: 82, duration: 0.062s, episode steps: 25, steps per second: 400, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.033 [-1.141, 1.624], loss: 3.930235, mean_absolute_error: 8.306928, mean_q: 15.581227, mean_eps: 0.830615\n",
      "  1815/50000: episode: 83, duration: 0.048s, episode steps: 19, steps per second: 392, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: 0.062 [-0.601, 0.994], loss: 4.138751, mean_absolute_error: 8.428285, mean_q: 15.672112, mean_eps: 0.828525\n",
      "  1868/50000: episode: 84, duration: 0.129s, episode steps: 53, steps per second: 411, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.377 [0.000, 1.000], mean observation: -0.031 [-2.495, 3.233], loss: 4.644393, mean_absolute_error: 8.460590, mean_q: 15.682826, mean_eps: 0.825105\n",
      "  1896/50000: episode: 85, duration: 0.068s, episode steps: 28, steps per second: 412, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: 0.052 [-1.152, 1.731], loss: 3.063578, mean_absolute_error: 8.377138, mean_q: 15.930993, mean_eps: 0.821257\n",
      "  1922/50000: episode: 86, duration: 0.065s, episode steps: 26, steps per second: 398, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.042 [-1.219, 2.008], loss: 6.466627, mean_absolute_error: 8.690629, mean_q: 15.775889, mean_eps: 0.818692\n",
      "  1941/50000: episode: 87, duration: 0.047s, episode steps: 19, steps per second: 406, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.368 [0.000, 1.000], mean observation: 0.092 [-1.137, 1.998], loss: 3.534145, mean_absolute_error: 8.504792, mean_q: 15.775176, mean_eps: 0.816555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1954/50000: episode: 88, duration: 0.034s, episode steps: 13, steps per second: 378, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.105 [-0.946, 1.660], loss: 4.821831, mean_absolute_error: 8.597705, mean_q: 16.073599, mean_eps: 0.815035\n",
      "  2000/50000: episode: 89, duration: 0.131s, episode steps: 46, steps per second: 350, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.543 [0.000, 1.000], mean observation: -0.050 [-1.607, 0.770], loss: 4.187607, mean_absolute_error: 8.590580, mean_q: 16.166035, mean_eps: 0.812232\n",
      "  2010/50000: episode: 90, duration: 0.027s, episode steps: 10, steps per second: 373, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: -0.133 [-1.669, 0.952], loss: 5.219007, mean_absolute_error: 8.752424, mean_q: 16.107631, mean_eps: 0.809573\n",
      "  2055/50000: episode: 91, duration: 0.111s, episode steps: 45, steps per second: 407, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: 0.038 [-0.791, 1.692], loss: 5.175670, mean_absolute_error: 8.759348, mean_q: 16.244319, mean_eps: 0.806960\n",
      "  2081/50000: episode: 92, duration: 0.064s, episode steps: 26, steps per second: 404, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: 0.022 [-0.975, 1.398], loss: 4.322356, mean_absolute_error: 8.699847, mean_q: 16.328858, mean_eps: 0.803588\n",
      "  2094/50000: episode: 93, duration: 0.033s, episode steps: 13, steps per second: 393, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.108 [-1.135, 1.786], loss: 5.125196, mean_absolute_error: 8.769468, mean_q: 16.363508, mean_eps: 0.801735\n",
      "  2110/50000: episode: 94, duration: 0.042s, episode steps: 16, steps per second: 385, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.080 [-0.819, 1.208], loss: 3.283261, mean_absolute_error: 8.681636, mean_q: 16.378292, mean_eps: 0.800358\n",
      "  2136/50000: episode: 95, duration: 0.068s, episode steps: 26, steps per second: 381, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.577 [0.000, 1.000], mean observation: -0.051 [-1.503, 0.955], loss: 3.381434, mean_absolute_error: 8.775237, mean_q: 16.650417, mean_eps: 0.798363\n",
      "  2158/50000: episode: 96, duration: 0.057s, episode steps: 22, steps per second: 384, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.409 [0.000, 1.000], mean observation: 0.095 [-0.791, 1.683], loss: 3.945221, mean_absolute_error: 8.808827, mean_q: 16.677313, mean_eps: 0.796083\n",
      "  2174/50000: episode: 97, duration: 0.043s, episode steps: 16, steps per second: 376, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.375 [0.000, 1.000], mean observation: 0.102 [-0.772, 1.515], loss: 3.999711, mean_absolute_error: 8.774890, mean_q: 16.608033, mean_eps: 0.794277\n",
      "  2188/50000: episode: 98, duration: 0.035s, episode steps: 14, steps per second: 400, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.786 [0.000, 1.000], mean observation: -0.100 [-2.514, 1.589], loss: 3.876172, mean_absolute_error: 8.735379, mean_q: 16.543124, mean_eps: 0.792852\n",
      "  2227/50000: episode: 99, duration: 0.097s, episode steps: 39, steps per second: 403, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: 0.010 [-0.995, 1.280], loss: 3.972383, mean_absolute_error: 8.819135, mean_q: 16.573401, mean_eps: 0.790335\n",
      "  2255/50000: episode: 100, duration: 0.070s, episode steps: 28, steps per second: 403, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.607 [0.000, 1.000], mean observation: -0.109 [-2.274, 1.145], loss: 3.412084, mean_absolute_error: 8.813366, mean_q: 16.738018, mean_eps: 0.787153\n",
      "  2300/50000: episode: 101, duration: 0.112s, episode steps: 45, steps per second: 401, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: -0.081 [-1.696, 0.780], loss: 3.797507, mean_absolute_error: 8.860464, mean_q: 16.876122, mean_eps: 0.783685\n",
      "  2323/50000: episode: 102, duration: 0.059s, episode steps: 23, steps per second: 387, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.348 [0.000, 1.000], mean observation: 0.068 [-1.372, 2.389], loss: 5.104778, mean_absolute_error: 9.040217, mean_q: 16.834282, mean_eps: 0.780455\n",
      "  2355/50000: episode: 103, duration: 0.079s, episode steps: 32, steps per second: 406, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.594 [0.000, 1.000], mean observation: -0.013 [-2.025, 1.388], loss: 3.661191, mean_absolute_error: 8.998507, mean_q: 17.111251, mean_eps: 0.777842\n",
      "  2379/50000: episode: 104, duration: 0.059s, episode steps: 24, steps per second: 407, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.075 [-2.676, 1.580], loss: 3.096678, mean_absolute_error: 8.991677, mean_q: 17.232061, mean_eps: 0.775182\n",
      "  2400/50000: episode: 105, duration: 0.055s, episode steps: 21, steps per second: 381, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.050 [-2.317, 1.416], loss: 5.116510, mean_absolute_error: 9.074977, mean_q: 17.082141, mean_eps: 0.773045\n",
      "  2421/50000: episode: 106, duration: 0.057s, episode steps: 21, steps per second: 367, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: 0.095 [-0.571, 1.165], loss: 5.772040, mean_absolute_error: 9.194070, mean_q: 16.956299, mean_eps: 0.771050\n",
      "  2433/50000: episode: 107, duration: 0.034s, episode steps: 12, steps per second: 355, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.098 [-1.828, 1.148], loss: 4.395975, mean_absolute_error: 9.135808, mean_q: 17.094126, mean_eps: 0.769482\n",
      "  2465/50000: episode: 108, duration: 0.080s, episode steps: 32, steps per second: 398, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: -0.081 [-1.363, 0.587], loss: 4.650563, mean_absolute_error: 9.100557, mean_q: 17.151806, mean_eps: 0.767393\n",
      "  2493/50000: episode: 109, duration: 0.069s, episode steps: 28, steps per second: 409, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.536 [0.000, 1.000], mean observation: -0.099 [-1.581, 0.749], loss: 3.731456, mean_absolute_error: 9.107867, mean_q: 17.267313, mean_eps: 0.764543\n",
      "  2530/50000: episode: 110, duration: 0.097s, episode steps: 37, steps per second: 383, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.432 [0.000, 1.000], mean observation: 0.036 [-1.195, 1.972], loss: 3.614575, mean_absolute_error: 9.207804, mean_q: 17.458268, mean_eps: 0.761455\n",
      "  2549/50000: episode: 111, duration: 0.054s, episode steps: 19, steps per second: 355, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.421 [0.000, 1.000], mean observation: 0.085 [-0.806, 1.348], loss: 5.102099, mean_absolute_error: 9.319876, mean_q: 17.561360, mean_eps: 0.758795\n",
      "  2569/50000: episode: 112, duration: 0.059s, episode steps: 20, steps per second: 340, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.085 [-0.807, 1.177], loss: 3.901244, mean_absolute_error: 9.198898, mean_q: 17.469865, mean_eps: 0.756943\n",
      "  2583/50000: episode: 113, duration: 0.039s, episode steps: 14, steps per second: 363, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.092 [-0.980, 1.576], loss: 4.519367, mean_absolute_error: 9.221278, mean_q: 17.459047, mean_eps: 0.755327\n",
      "  2604/50000: episode: 114, duration: 0.057s, episode steps: 21, steps per second: 368, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.102 [-0.559, 1.185], loss: 5.504140, mean_absolute_error: 9.302436, mean_q: 17.359444, mean_eps: 0.753665\n",
      "  2637/50000: episode: 115, duration: 0.081s, episode steps: 33, steps per second: 408, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: -0.077 [-1.719, 0.955], loss: 3.723169, mean_absolute_error: 9.235143, mean_q: 17.485119, mean_eps: 0.751100\n",
      "  2649/50000: episode: 116, duration: 0.031s, episode steps: 12, steps per second: 387, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.114 [-1.304, 0.773], loss: 3.940278, mean_absolute_error: 9.189867, mean_q: 17.470079, mean_eps: 0.748963\n",
      "  2668/50000: episode: 117, duration: 0.048s, episode steps: 19, steps per second: 398, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.579 [0.000, 1.000], mean observation: -0.068 [-1.343, 0.817], loss: 4.348877, mean_absolute_error: 9.252136, mean_q: 17.534319, mean_eps: 0.747490\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2697/50000: episode: 118, duration: 0.074s, episode steps: 29, steps per second: 392, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.088 [-0.741, 1.145], loss: 4.803394, mean_absolute_error: 9.274438, mean_q: 17.489754, mean_eps: 0.745210\n",
      "  2730/50000: episode: 119, duration: 0.093s, episode steps: 33, steps per second: 354, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: 0.047 [-0.992, 1.795], loss: 4.985629, mean_absolute_error: 9.346875, mean_q: 17.508879, mean_eps: 0.742265\n",
      "  2754/50000: episode: 120, duration: 0.111s, episode steps: 24, steps per second: 217, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.103 [-0.950, 0.442], loss: 3.446511, mean_absolute_error: 9.304729, mean_q: 17.716523, mean_eps: 0.739558\n",
      "  2786/50000: episode: 121, duration: 0.120s, episode steps: 32, steps per second: 266, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.061 [-0.778, 1.279], loss: 4.584796, mean_absolute_error: 9.421358, mean_q: 17.873696, mean_eps: 0.736898\n",
      "  2813/50000: episode: 122, duration: 0.082s, episode steps: 27, steps per second: 329, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.407 [0.000, 1.000], mean observation: 0.034 [-1.014, 1.657], loss: 4.816734, mean_absolute_error: 9.414358, mean_q: 17.655726, mean_eps: 0.734095\n",
      "  2839/50000: episode: 123, duration: 0.063s, episode steps: 26, steps per second: 411, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.062 [-1.154, 2.086], loss: 4.687922, mean_absolute_error: 9.412272, mean_q: 17.821286, mean_eps: 0.731578\n",
      "  2861/50000: episode: 124, duration: 0.055s, episode steps: 22, steps per second: 402, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.409 [0.000, 1.000], mean observation: 0.048 [-0.963, 1.592], loss: 4.234387, mean_absolute_error: 9.380440, mean_q: 17.772492, mean_eps: 0.729298\n",
      "  2912/50000: episode: 125, duration: 0.133s, episode steps: 51, steps per second: 384, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.067 [-0.796, 0.918], loss: 4.474643, mean_absolute_error: 9.447647, mean_q: 17.842259, mean_eps: 0.725830\n",
      "  2974/50000: episode: 126, duration: 0.165s, episode steps: 62, steps per second: 376, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.179 [-1.636, 0.635], loss: 3.802366, mean_absolute_error: 9.524223, mean_q: 18.190653, mean_eps: 0.720462\n",
      "  3003/50000: episode: 127, duration: 0.073s, episode steps: 29, steps per second: 396, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.098 [-1.032, 0.390], loss: 5.206344, mean_absolute_error: 9.575829, mean_q: 18.044441, mean_eps: 0.716140\n",
      "  3039/50000: episode: 128, duration: 0.088s, episode steps: 36, steps per second: 409, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: 0.107 [-0.398, 0.974], loss: 4.277669, mean_absolute_error: 9.557995, mean_q: 18.133150, mean_eps: 0.713053\n",
      "  3093/50000: episode: 129, duration: 0.138s, episode steps: 54, steps per second: 392, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.537 [0.000, 1.000], mean observation: -0.060 [-2.033, 0.786], loss: 3.821580, mean_absolute_error: 9.600098, mean_q: 18.343593, mean_eps: 0.708777\n",
      "  3106/50000: episode: 130, duration: 0.037s, episode steps: 13, steps per second: 351, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: -0.121 [-1.281, 0.736], loss: 4.291304, mean_absolute_error: 9.679507, mean_q: 18.368905, mean_eps: 0.705595\n",
      "  3155/50000: episode: 131, duration: 0.119s, episode steps: 49, steps per second: 412, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: 0.069 [-0.615, 1.448], loss: 4.663713, mean_absolute_error: 9.735887, mean_q: 18.449144, mean_eps: 0.702650\n",
      "  3179/50000: episode: 132, duration: 0.061s, episode steps: 24, steps per second: 390, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.085 [-0.567, 0.943], loss: 3.991477, mean_absolute_error: 9.701469, mean_q: 18.484905, mean_eps: 0.699183\n",
      "  3273/50000: episode: 133, duration: 0.243s, episode steps: 94, steps per second: 387, episode reward: 94.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: 0.046 [-1.367, 1.349], loss: 3.680263, mean_absolute_error: 9.893870, mean_q: 18.957859, mean_eps: 0.693578\n",
      "  3290/50000: episode: 134, duration: 0.067s, episode steps: 17, steps per second: 255, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.412 [0.000, 1.000], mean observation: 0.112 [-0.634, 1.428], loss: 4.182926, mean_absolute_error: 9.993408, mean_q: 19.115892, mean_eps: 0.688305\n",
      "  3322/50000: episode: 135, duration: 0.116s, episode steps: 32, steps per second: 277, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.085 [-0.849, 1.942], loss: 4.247526, mean_absolute_error: 10.026823, mean_q: 19.052226, mean_eps: 0.685978\n",
      "  3366/50000: episode: 136, duration: 0.155s, episode steps: 44, steps per second: 285, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.012 [-0.894, 0.725], loss: 4.792142, mean_absolute_error: 10.115925, mean_q: 19.287596, mean_eps: 0.682368\n",
      "  3403/50000: episode: 137, duration: 0.097s, episode steps: 37, steps per second: 380, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.052 [-0.987, 0.568], loss: 5.274594, mean_absolute_error: 10.147759, mean_q: 19.247109, mean_eps: 0.678520\n",
      "  3435/50000: episode: 138, duration: 0.080s, episode steps: 32, steps per second: 401, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: 0.126 [-0.554, 1.551], loss: 4.171353, mean_absolute_error: 10.133762, mean_q: 19.326598, mean_eps: 0.675243\n",
      "  3458/50000: episode: 139, duration: 0.076s, episode steps: 23, steps per second: 303, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.067 [-0.959, 1.366], loss: 5.687396, mean_absolute_error: 10.292803, mean_q: 19.569523, mean_eps: 0.672630\n",
      "  3471/50000: episode: 140, duration: 0.042s, episode steps: 13, steps per second: 308, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.101 [-1.849, 1.034], loss: 3.915807, mean_absolute_error: 10.147189, mean_q: 19.463080, mean_eps: 0.670920\n",
      "  3496/50000: episode: 141, duration: 0.064s, episode steps: 25, steps per second: 389, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: -0.084 [-1.239, 0.427], loss: 5.097801, mean_absolute_error: 10.117332, mean_q: 19.286723, mean_eps: 0.669115\n",
      "  3514/50000: episode: 142, duration: 0.047s, episode steps: 18, steps per second: 380, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.611 [0.000, 1.000], mean observation: -0.099 [-1.663, 0.798], loss: 4.402722, mean_absolute_error: 10.262061, mean_q: 19.593989, mean_eps: 0.667072\n",
      "  3548/50000: episode: 143, duration: 0.089s, episode steps: 34, steps per second: 380, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.108 [-0.420, 1.629], loss: 4.725061, mean_absolute_error: 10.365874, mean_q: 19.824143, mean_eps: 0.664602\n",
      "  3572/50000: episode: 144, duration: 0.062s, episode steps: 24, steps per second: 386, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.036 [-1.402, 1.000], loss: 6.423488, mean_absolute_error: 10.362441, mean_q: 19.449907, mean_eps: 0.661848\n",
      "  3597/50000: episode: 145, duration: 0.066s, episode steps: 25, steps per second: 379, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.089 [-1.957, 0.988], loss: 4.296053, mean_absolute_error: 10.320475, mean_q: 19.634935, mean_eps: 0.659520\n",
      "  3622/50000: episode: 146, duration: 0.066s, episode steps: 25, steps per second: 378, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: -0.051 [-1.171, 0.645], loss: 4.578032, mean_absolute_error: 10.404833, mean_q: 19.859001, mean_eps: 0.657145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  3664/50000: episode: 147, duration: 0.108s, episode steps: 42, steps per second: 390, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.042 [-0.999, 1.321], loss: 3.888794, mean_absolute_error: 10.449230, mean_q: 20.052266, mean_eps: 0.653962\n",
      "  3707/50000: episode: 148, duration: 0.112s, episode steps: 43, steps per second: 383, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: 0.042 [-0.804, 1.157], loss: 5.417473, mean_absolute_error: 10.546180, mean_q: 20.071957, mean_eps: 0.649925\n",
      "  3763/50000: episode: 149, duration: 0.145s, episode steps: 56, steps per second: 387, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.100 [-0.986, 0.538], loss: 4.936752, mean_absolute_error: 10.527509, mean_q: 20.103037, mean_eps: 0.645223\n",
      "  3785/50000: episode: 150, duration: 0.058s, episode steps: 22, steps per second: 382, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.116 [-0.425, 0.889], loss: 4.116650, mean_absolute_error: 10.477539, mean_q: 20.054719, mean_eps: 0.641518\n",
      "  3813/50000: episode: 151, duration: 0.072s, episode steps: 28, steps per second: 390, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.536 [0.000, 1.000], mean observation: 0.108 [-0.570, 1.004], loss: 4.288724, mean_absolute_error: 10.596736, mean_q: 20.262992, mean_eps: 0.639143\n",
      "  3838/50000: episode: 152, duration: 0.061s, episode steps: 25, steps per second: 407, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: -0.107 [-0.966, 0.369], loss: 4.018382, mean_absolute_error: 10.785410, mean_q: 20.706887, mean_eps: 0.636625\n",
      "  3852/50000: episode: 153, duration: 0.037s, episode steps: 14, steps per second: 375, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.095 [-1.625, 0.942], loss: 4.695829, mean_absolute_error: 10.816300, mean_q: 20.756429, mean_eps: 0.634772\n",
      "  3875/50000: episode: 154, duration: 0.057s, episode steps: 23, steps per second: 401, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: 0.070 [-0.956, 1.403], loss: 5.357892, mean_absolute_error: 10.771205, mean_q: 20.580630, mean_eps: 0.633015\n",
      "  3926/50000: episode: 155, duration: 0.132s, episode steps: 51, steps per second: 387, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.549 [0.000, 1.000], mean observation: -0.048 [-1.892, 0.953], loss: 5.476888, mean_absolute_error: 10.861932, mean_q: 20.716006, mean_eps: 0.629500\n",
      "  3956/50000: episode: 156, duration: 0.079s, episode steps: 30, steps per second: 380, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.433 [0.000, 1.000], mean observation: 0.055 [-0.795, 1.563], loss: 5.684549, mean_absolute_error: 10.920841, mean_q: 20.859290, mean_eps: 0.625652\n",
      "  3997/50000: episode: 157, duration: 0.106s, episode steps: 41, steps per second: 388, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: -0.126 [-0.938, 0.464], loss: 4.632071, mean_absolute_error: 10.842951, mean_q: 20.737880, mean_eps: 0.622280\n",
      "  4005/50000: episode: 158, duration: 0.024s, episode steps: 8, steps per second: 336, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.154 [-1.145, 1.911], loss: 5.077367, mean_absolute_error: 10.879853, mean_q: 20.783718, mean_eps: 0.619953\n",
      "  4069/50000: episode: 159, duration: 0.164s, episode steps: 64, steps per second: 390, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.007 [-1.417, 1.313], loss: 4.152279, mean_absolute_error: 11.034252, mean_q: 21.210030, mean_eps: 0.616533\n",
      "  4102/50000: episode: 160, duration: 0.087s, episode steps: 33, steps per second: 378, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.099 [-0.559, 1.036], loss: 5.450044, mean_absolute_error: 11.157920, mean_q: 21.337773, mean_eps: 0.611925\n",
      "  4123/50000: episode: 161, duration: 0.056s, episode steps: 21, steps per second: 376, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.109 [-0.605, 1.529], loss: 4.088446, mean_absolute_error: 11.125787, mean_q: 21.361855, mean_eps: 0.609360\n",
      "  4152/50000: episode: 162, duration: 0.076s, episode steps: 29, steps per second: 382, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: -0.135 [-1.440, 0.398], loss: 5.414019, mean_absolute_error: 11.282515, mean_q: 21.636060, mean_eps: 0.606985\n",
      "  4196/50000: episode: 163, duration: 0.114s, episode steps: 44, steps per second: 387, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.523 [0.000, 1.000], mean observation: 0.105 [-0.587, 0.855], loss: 5.026258, mean_absolute_error: 11.232113, mean_q: 21.563594, mean_eps: 0.603518\n",
      "  4215/50000: episode: 164, duration: 0.051s, episode steps: 19, steps per second: 373, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.632 [0.000, 1.000], mean observation: -0.072 [-1.788, 0.983], loss: 5.047964, mean_absolute_error: 11.178537, mean_q: 21.406016, mean_eps: 0.600525\n",
      "  4234/50000: episode: 165, duration: 0.050s, episode steps: 19, steps per second: 381, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.089 [-0.741, 1.130], loss: 5.174174, mean_absolute_error: 11.423121, mean_q: 21.943322, mean_eps: 0.598720\n",
      "  4270/50000: episode: 166, duration: 0.093s, episode steps: 36, steps per second: 388, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: 0.109 [-0.567, 1.048], loss: 4.849177, mean_absolute_error: 11.441478, mean_q: 22.007722, mean_eps: 0.596108\n",
      "  4287/50000: episode: 167, duration: 0.045s, episode steps: 17, steps per second: 380, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: -0.078 [-1.483, 1.003], loss: 5.561340, mean_absolute_error: 11.495799, mean_q: 22.037891, mean_eps: 0.593590\n",
      "  4309/50000: episode: 168, duration: 0.058s, episode steps: 22, steps per second: 379, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: 0.056 [-0.654, 1.323], loss: 4.951764, mean_absolute_error: 11.471650, mean_q: 22.011024, mean_eps: 0.591738\n",
      "  4348/50000: episode: 169, duration: 0.105s, episode steps: 39, steps per second: 371, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: 0.073 [-0.625, 1.158], loss: 6.568002, mean_absolute_error: 11.639214, mean_q: 22.104954, mean_eps: 0.588840\n",
      "  4439/50000: episode: 170, duration: 0.235s, episode steps: 91, steps per second: 387, episode reward: 91.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.082 [-0.679, 1.317], loss: 5.785131, mean_absolute_error: 11.659040, mean_q: 22.359853, mean_eps: 0.582665\n",
      "  4486/50000: episode: 171, duration: 0.120s, episode steps: 47, steps per second: 393, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: -0.082 [-1.090, 0.772], loss: 6.275177, mean_absolute_error: 11.814148, mean_q: 22.633078, mean_eps: 0.576110\n",
      "  4523/50000: episode: 172, duration: 0.098s, episode steps: 37, steps per second: 378, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.076 [-0.431, 1.141], loss: 5.802389, mean_absolute_error: 11.781092, mean_q: 22.565138, mean_eps: 0.572120\n",
      "  4565/50000: episode: 173, duration: 0.108s, episode steps: 42, steps per second: 387, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.127 [-1.125, 0.270], loss: 5.255110, mean_absolute_error: 11.936677, mean_q: 23.075746, mean_eps: 0.568368\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  4622/50000: episode: 174, duration: 0.156s, episode steps: 57, steps per second: 365, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: 0.098 [-0.770, 1.088], loss: 6.055066, mean_absolute_error: 11.960460, mean_q: 22.961523, mean_eps: 0.563665\n",
      "  4681/50000: episode: 175, duration: 0.168s, episode steps: 59, steps per second: 352, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.492 [0.000, 1.000], mean observation: -0.100 [-1.145, 0.526], loss: 4.975235, mean_absolute_error: 12.012402, mean_q: 23.194024, mean_eps: 0.558155\n",
      "  4696/50000: episode: 176, duration: 0.046s, episode steps: 15, steps per second: 329, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.078 [-0.825, 1.448], loss: 3.997256, mean_absolute_error: 12.070277, mean_q: 23.360219, mean_eps: 0.554640\n",
      "  4783/50000: episode: 177, duration: 0.232s, episode steps: 87, steps per second: 375, episode reward: 87.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.506 [0.000, 1.000], mean observation: 0.088 [-1.016, 1.248], loss: 6.200742, mean_absolute_error: 12.252645, mean_q: 23.589339, mean_eps: 0.549795\n",
      "  4817/50000: episode: 178, duration: 0.088s, episode steps: 34, steps per second: 385, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.110 [-0.480, 0.870], loss: 6.349963, mean_absolute_error: 12.276096, mean_q: 23.505829, mean_eps: 0.544048\n",
      "  4861/50000: episode: 179, duration: 0.113s, episode steps: 44, steps per second: 390, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.107 [-0.444, 1.504], loss: 5.851773, mean_absolute_error: 12.359527, mean_q: 23.841567, mean_eps: 0.540343\n",
      "  4886/50000: episode: 180, duration: 0.063s, episode steps: 25, steps per second: 397, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.096 [-0.610, 0.926], loss: 5.947074, mean_absolute_error: 12.368538, mean_q: 23.839223, mean_eps: 0.537065\n",
      "  4978/50000: episode: 181, duration: 0.231s, episode steps: 92, steps per second: 398, episode reward: 92.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: 0.025 [-0.619, 1.056], loss: 6.079464, mean_absolute_error: 12.434089, mean_q: 23.917103, mean_eps: 0.531508\n",
      "  5019/50000: episode: 182, duration: 0.102s, episode steps: 41, steps per second: 401, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: -0.090 [-1.246, 0.613], loss: 5.750530, mean_absolute_error: 12.456052, mean_q: 24.056980, mean_eps: 0.525190\n",
      "  5045/50000: episode: 183, duration: 0.065s, episode steps: 26, steps per second: 400, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.096 [-0.556, 1.024], loss: 5.594341, mean_absolute_error: 12.581509, mean_q: 24.363483, mean_eps: 0.522008\n",
      "  5085/50000: episode: 184, duration: 0.111s, episode steps: 40, steps per second: 359, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.086 [-0.948, 0.577], loss: 6.627297, mean_absolute_error: 12.619040, mean_q: 24.392305, mean_eps: 0.518873\n",
      "  5154/50000: episode: 185, duration: 0.184s, episode steps: 69, steps per second: 376, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.493 [0.000, 1.000], mean observation: -0.065 [-1.336, 0.577], loss: 6.582746, mean_absolute_error: 12.782185, mean_q: 24.613241, mean_eps: 0.513695\n",
      "  5194/50000: episode: 186, duration: 0.104s, episode steps: 40, steps per second: 384, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.047 [-1.129, 0.621], loss: 6.226365, mean_absolute_error: 12.817143, mean_q: 24.881741, mean_eps: 0.508518\n",
      "  5208/50000: episode: 187, duration: 0.038s, episode steps: 14, steps per second: 366, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.109 [-1.449, 0.811], loss: 7.830588, mean_absolute_error: 12.926309, mean_q: 24.705885, mean_eps: 0.505953\n",
      "  5248/50000: episode: 188, duration: 0.102s, episode steps: 40, steps per second: 393, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.119 [-0.415, 1.183], loss: 6.639786, mean_absolute_error: 13.041963, mean_q: 25.059970, mean_eps: 0.503387\n",
      "  5281/50000: episode: 189, duration: 0.083s, episode steps: 33, steps per second: 396, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: 0.113 [-0.608, 1.809], loss: 5.629858, mean_absolute_error: 13.121681, mean_q: 25.425628, mean_eps: 0.499920\n",
      "  5309/50000: episode: 190, duration: 0.071s, episode steps: 28, steps per second: 394, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.536 [0.000, 1.000], mean observation: 0.113 [-0.428, 0.774], loss: 7.039352, mean_absolute_error: 13.129678, mean_q: 25.399353, mean_eps: 0.497022\n",
      "  5335/50000: episode: 191, duration: 0.064s, episode steps: 26, steps per second: 404, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: -0.109 [-1.553, 0.664], loss: 6.715670, mean_absolute_error: 13.125909, mean_q: 25.346787, mean_eps: 0.494458\n",
      "  5369/50000: episode: 192, duration: 0.086s, episode steps: 34, steps per second: 397, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.121 [-1.030, 0.683], loss: 5.729300, mean_absolute_error: 13.121453, mean_q: 25.423800, mean_eps: 0.491608\n",
      "  5385/50000: episode: 193, duration: 0.040s, episode steps: 16, steps per second: 398, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.375 [0.000, 1.000], mean observation: 0.103 [-0.983, 1.820], loss: 6.227202, mean_absolute_error: 13.154444, mean_q: 25.473338, mean_eps: 0.489233\n",
      "  5421/50000: episode: 194, duration: 0.091s, episode steps: 36, steps per second: 394, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: 0.126 [-0.440, 0.724], loss: 6.282182, mean_absolute_error: 13.304315, mean_q: 25.732987, mean_eps: 0.486762\n",
      "  5448/50000: episode: 195, duration: 0.073s, episode steps: 27, steps per second: 368, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.118 [-0.899, 0.391], loss: 7.667343, mean_absolute_error: 13.411868, mean_q: 25.878100, mean_eps: 0.483770\n",
      "  5513/50000: episode: 196, duration: 0.165s, episode steps: 65, steps per second: 395, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.523 [0.000, 1.000], mean observation: 0.099 [-0.606, 0.939], loss: 7.688758, mean_absolute_error: 13.466692, mean_q: 25.917629, mean_eps: 0.479400\n",
      "  5543/50000: episode: 197, duration: 0.090s, episode steps: 30, steps per second: 332, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.114 [-1.022, 0.378], loss: 5.552478, mean_absolute_error: 13.405877, mean_q: 25.933484, mean_eps: 0.474888\n",
      "  5591/50000: episode: 198, duration: 0.124s, episode steps: 48, steps per second: 387, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.521 [0.000, 1.000], mean observation: -0.048 [-1.330, 0.796], loss: 5.717451, mean_absolute_error: 13.603534, mean_q: 26.441583, mean_eps: 0.471183\n",
      "  5617/50000: episode: 199, duration: 0.068s, episode steps: 26, steps per second: 384, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.099 [-0.993, 0.372], loss: 8.473291, mean_absolute_error: 13.669552, mean_q: 26.322873, mean_eps: 0.467668\n",
      "  5658/50000: episode: 200, duration: 0.134s, episode steps: 41, steps per second: 307, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: -0.105 [-1.062, 0.535], loss: 6.581184, mean_absolute_error: 13.713005, mean_q: 26.616138, mean_eps: 0.464485\n",
      "  5694/50000: episode: 201, duration: 0.131s, episode steps: 36, steps per second: 276, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.087 [-0.801, 1.871], loss: 4.862792, mean_absolute_error: 13.773191, mean_q: 26.793840, mean_eps: 0.460828\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  5738/50000: episode: 202, duration: 0.148s, episode steps: 44, steps per second: 297, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: -0.089 [-0.916, 0.575], loss: 8.152367, mean_absolute_error: 13.935198, mean_q: 26.825813, mean_eps: 0.457028\n",
      "  5858/50000: episode: 203, duration: 0.334s, episode steps: 120, steps per second: 360, episode reward: 120.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.034 [-0.728, 1.557], loss: 7.179053, mean_absolute_error: 14.055230, mean_q: 27.227070, mean_eps: 0.449238\n",
      "  5880/50000: episode: 204, duration: 0.068s, episode steps: 22, steps per second: 323, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.115 [-0.985, 0.359], loss: 6.984232, mean_absolute_error: 14.028120, mean_q: 27.220686, mean_eps: 0.442493\n",
      "  5947/50000: episode: 205, duration: 0.201s, episode steps: 67, steps per second: 333, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.493 [0.000, 1.000], mean observation: 0.019 [-0.976, 1.223], loss: 8.615420, mean_absolute_error: 14.086565, mean_q: 27.059646, mean_eps: 0.438265\n",
      "  6009/50000: episode: 206, duration: 0.164s, episode steps: 62, steps per second: 378, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.122 [-0.846, 0.396], loss: 6.948487, mean_absolute_error: 14.117261, mean_q: 27.344714, mean_eps: 0.432138\n",
      "  6055/50000: episode: 207, duration: 0.153s, episode steps: 46, steps per second: 301, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.056 [-0.800, 1.173], loss: 6.545182, mean_absolute_error: 14.290024, mean_q: 27.671640, mean_eps: 0.427007\n",
      "  6084/50000: episode: 208, duration: 0.093s, episode steps: 29, steps per second: 312, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: 0.099 [-0.415, 1.086], loss: 7.635299, mean_absolute_error: 14.371732, mean_q: 27.762835, mean_eps: 0.423445\n",
      "  6124/50000: episode: 209, duration: 0.105s, episode steps: 40, steps per second: 383, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.121 [-0.828, 0.528], loss: 5.475707, mean_absolute_error: 14.351590, mean_q: 27.847912, mean_eps: 0.420168\n",
      "  6152/50000: episode: 210, duration: 0.077s, episode steps: 28, steps per second: 363, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: -0.127 [-1.184, 0.742], loss: 7.104509, mean_absolute_error: 14.440894, mean_q: 28.037120, mean_eps: 0.416937\n",
      "  6198/50000: episode: 211, duration: 0.142s, episode steps: 46, steps per second: 324, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.065 [-1.161, 0.557], loss: 7.134076, mean_absolute_error: 14.417351, mean_q: 27.963917, mean_eps: 0.413423\n",
      "  6224/50000: episode: 212, duration: 0.086s, episode steps: 26, steps per second: 304, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.156 [-1.004, 0.545], loss: 8.356742, mean_absolute_error: 14.584818, mean_q: 28.084540, mean_eps: 0.410003\n",
      "  6250/50000: episode: 213, duration: 0.086s, episode steps: 26, steps per second: 302, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.142 [-0.559, 0.891], loss: 10.348629, mean_absolute_error: 14.631919, mean_q: 28.084780, mean_eps: 0.407533\n",
      "  6314/50000: episode: 214, duration: 0.219s, episode steps: 64, steps per second: 292, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.136 [-0.903, 0.477], loss: 7.057834, mean_absolute_error: 14.592053, mean_q: 28.148883, mean_eps: 0.403258\n",
      "  6357/50000: episode: 215, duration: 0.137s, episode steps: 43, steps per second: 314, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: -0.098 [-0.881, 0.391], loss: 7.684144, mean_absolute_error: 14.759875, mean_q: 28.549040, mean_eps: 0.398175\n",
      "  6401/50000: episode: 216, duration: 0.142s, episode steps: 44, steps per second: 310, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.096 [-0.379, 0.987], loss: 7.000648, mean_absolute_error: 14.756605, mean_q: 28.505104, mean_eps: 0.394043\n",
      "  6424/50000: episode: 217, duration: 0.072s, episode steps: 23, steps per second: 320, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.565 [0.000, 1.000], mean observation: 0.133 [-0.942, 1.510], loss: 8.904773, mean_absolute_error: 14.853613, mean_q: 28.634618, mean_eps: 0.390860\n",
      "  6492/50000: episode: 218, duration: 0.188s, episode steps: 68, steps per second: 362, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.108 [-0.917, 0.663], loss: 8.364850, mean_absolute_error: 14.889120, mean_q: 28.767900, mean_eps: 0.386538\n",
      "  6550/50000: episode: 219, duration: 0.183s, episode steps: 58, steps per second: 317, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.095 [-1.092, 0.470], loss: 8.953159, mean_absolute_error: 15.016119, mean_q: 28.927277, mean_eps: 0.380552\n",
      "  6612/50000: episode: 220, duration: 0.196s, episode steps: 62, steps per second: 316, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.103 [-0.905, 1.506], loss: 7.498287, mean_absolute_error: 14.931217, mean_q: 28.885906, mean_eps: 0.374853\n",
      "  6661/50000: episode: 221, duration: 0.137s, episode steps: 49, steps per second: 357, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: -0.098 [-1.418, 0.518], loss: 7.364241, mean_absolute_error: 15.112679, mean_q: 29.279805, mean_eps: 0.369580\n",
      "  6694/50000: episode: 222, duration: 0.084s, episode steps: 33, steps per second: 395, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.104 [-0.894, 0.357], loss: 5.307844, mean_absolute_error: 15.069372, mean_q: 29.459765, mean_eps: 0.365685\n",
      "  6737/50000: episode: 223, duration: 0.105s, episode steps: 43, steps per second: 410, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: -0.091 [-1.001, 0.493], loss: 8.045932, mean_absolute_error: 15.302150, mean_q: 29.724095, mean_eps: 0.362075\n",
      "  6765/50000: episode: 224, duration: 0.078s, episode steps: 28, steps per second: 357, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.025 [-1.220, 0.830], loss: 6.152588, mean_absolute_error: 15.312687, mean_q: 29.869835, mean_eps: 0.358702\n",
      "  6827/50000: episode: 225, duration: 0.192s, episode steps: 62, steps per second: 322, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.093 [-1.175, 0.542], loss: 7.462057, mean_absolute_error: 15.352163, mean_q: 29.830020, mean_eps: 0.354427\n",
      "  6902/50000: episode: 226, duration: 0.189s, episode steps: 75, steps per second: 396, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.493 [0.000, 1.000], mean observation: -0.064 [-0.985, 0.542], loss: 6.590767, mean_absolute_error: 15.370932, mean_q: 30.016485, mean_eps: 0.347920\n",
      "  6959/50000: episode: 227, duration: 0.139s, episode steps: 57, steps per second: 410, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.099 [-0.743, 0.386], loss: 7.681319, mean_absolute_error: 15.653135, mean_q: 30.426644, mean_eps: 0.341650\n",
      "  6994/50000: episode: 228, duration: 0.085s, episode steps: 35, steps per second: 411, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.457 [0.000, 1.000], mean observation: -0.142 [-0.673, 0.390], loss: 9.464102, mean_absolute_error: 15.675025, mean_q: 30.431431, mean_eps: 0.337280\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  7114/50000: episode: 229, duration: 0.340s, episode steps: 120, steps per second: 353, episode reward: 120.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: 0.054 [-0.734, 0.870], loss: 9.942266, mean_absolute_error: 15.795280, mean_q: 30.595701, mean_eps: 0.329918\n",
      "  7160/50000: episode: 230, duration: 0.129s, episode steps: 46, steps per second: 356, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.129 [-1.319, 0.456], loss: 7.985271, mean_absolute_error: 15.923508, mean_q: 31.033326, mean_eps: 0.322033\n",
      "  7205/50000: episode: 231, duration: 0.134s, episode steps: 45, steps per second: 335, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.141 [-0.744, 0.376], loss: 6.283641, mean_absolute_error: 15.912469, mean_q: 31.201680, mean_eps: 0.317710\n",
      "  7285/50000: episode: 232, duration: 0.254s, episode steps: 80, steps per second: 314, episode reward: 80.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.487 [0.000, 1.000], mean observation: -0.084 [-0.899, 0.287], loss: 9.200929, mean_absolute_error: 16.281675, mean_q: 31.653456, mean_eps: 0.311773\n",
      "  7343/50000: episode: 233, duration: 0.171s, episode steps: 58, steps per second: 338, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.466 [0.000, 1.000], mean observation: -0.119 [-0.830, 0.258], loss: 8.762005, mean_absolute_error: 16.311783, mean_q: 31.715602, mean_eps: 0.305218\n",
      "  7427/50000: episode: 234, duration: 0.231s, episode steps: 84, steps per second: 364, episode reward: 84.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.087 [-0.697, 0.420], loss: 8.807281, mean_absolute_error: 16.364370, mean_q: 31.769549, mean_eps: 0.298473\n",
      "  7503/50000: episode: 235, duration: 0.235s, episode steps: 76, steps per second: 323, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: 0.080 [-0.359, 0.770], loss: 9.017987, mean_absolute_error: 16.505212, mean_q: 32.106894, mean_eps: 0.290873\n",
      "  7551/50000: episode: 236, duration: 0.137s, episode steps: 48, steps per second: 351, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: 0.144 [-0.260, 0.764], loss: 9.505397, mean_absolute_error: 16.537415, mean_q: 32.043894, mean_eps: 0.284983\n",
      "  7634/50000: episode: 237, duration: 0.244s, episode steps: 83, steps per second: 340, episode reward: 83.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.518 [0.000, 1.000], mean observation: 0.124 [-0.525, 0.942], loss: 9.629968, mean_absolute_error: 16.643589, mean_q: 32.313400, mean_eps: 0.278760\n",
      "  7672/50000: episode: 238, duration: 0.098s, episode steps: 38, steps per second: 387, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.447 [0.000, 1.000], mean observation: -0.171 [-0.937, 0.572], loss: 11.452303, mean_absolute_error: 16.762556, mean_q: 32.445424, mean_eps: 0.273013\n",
      "  7716/50000: episode: 239, duration: 0.133s, episode steps: 44, steps per second: 330, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: 0.143 [-0.590, 0.914], loss: 8.434665, mean_absolute_error: 16.605828, mean_q: 32.328601, mean_eps: 0.269118\n",
      "  7774/50000: episode: 240, duration: 0.182s, episode steps: 58, steps per second: 319, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.105 [-0.593, 0.930], loss: 10.412874, mean_absolute_error: 16.843907, mean_q: 32.689430, mean_eps: 0.264273\n",
      "  7862/50000: episode: 241, duration: 0.271s, episode steps: 88, steps per second: 325, episode reward: 88.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.098 [-0.696, 1.199], loss: 9.630904, mean_absolute_error: 16.836909, mean_q: 32.719228, mean_eps: 0.257338\n",
      "  7927/50000: episode: 242, duration: 0.176s, episode steps: 65, steps per second: 370, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.523 [0.000, 1.000], mean observation: 0.114 [-0.512, 0.839], loss: 10.214850, mean_absolute_error: 16.914065, mean_q: 32.777384, mean_eps: 0.250070\n",
      "  8030/50000: episode: 243, duration: 0.298s, episode steps: 103, steps per second: 346, episode reward: 103.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.056 [-0.510, 0.984], loss: 10.254070, mean_absolute_error: 16.878214, mean_q: 32.789780, mean_eps: 0.242090\n",
      "  8090/50000: episode: 244, duration: 0.201s, episode steps: 60, steps per second: 298, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.097 [-0.355, 0.817], loss: 8.679021, mean_absolute_error: 17.000111, mean_q: 33.161663, mean_eps: 0.234348\n",
      "  8158/50000: episode: 245, duration: 0.215s, episode steps: 68, steps per second: 317, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.060 [-0.699, 1.184], loss: 9.479506, mean_absolute_error: 17.103517, mean_q: 33.261794, mean_eps: 0.228268\n",
      "  8246/50000: episode: 246, duration: 0.240s, episode steps: 88, steps per second: 366, episode reward: 88.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.523 [0.000, 1.000], mean observation: 0.090 [-0.604, 0.839], loss: 8.200446, mean_absolute_error: 17.177935, mean_q: 33.529869, mean_eps: 0.220858\n",
      "  8319/50000: episode: 247, duration: 0.227s, episode steps: 73, steps per second: 322, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: -0.120 [-0.827, 0.659], loss: 9.310509, mean_absolute_error: 17.253992, mean_q: 33.624993, mean_eps: 0.213210\n",
      "  8392/50000: episode: 248, duration: 0.190s, episode steps: 73, steps per second: 384, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.534 [0.000, 1.000], mean observation: 0.139 [-0.625, 0.860], loss: 7.604354, mean_absolute_error: 17.367565, mean_q: 34.068230, mean_eps: 0.206275\n",
      "  8466/50000: episode: 249, duration: 0.236s, episode steps: 74, steps per second: 313, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.527 [0.000, 1.000], mean observation: 0.133 [-0.401, 0.734], loss: 10.150248, mean_absolute_error: 17.391817, mean_q: 33.893582, mean_eps: 0.199293\n",
      "  8571/50000: episode: 250, duration: 0.259s, episode steps: 105, steps per second: 406, episode reward: 105.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.111 [-0.520, 0.897], loss: 11.245770, mean_absolute_error: 17.566557, mean_q: 34.163264, mean_eps: 0.190790\n",
      "  8645/50000: episode: 251, duration: 0.183s, episode steps: 74, steps per second: 404, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.527 [0.000, 1.000], mean observation: 0.130 [-0.376, 0.934], loss: 10.451615, mean_absolute_error: 17.658900, mean_q: 34.365024, mean_eps: 0.182288\n",
      "  8695/50000: episode: 252, duration: 0.147s, episode steps: 50, steps per second: 341, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.540 [0.000, 1.000], mean observation: 0.133 [-0.412, 0.720], loss: 10.302897, mean_absolute_error: 17.724734, mean_q: 34.528484, mean_eps: 0.176398\n",
      "  8751/50000: episode: 253, duration: 0.138s, episode steps: 56, steps per second: 406, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.536 [0.000, 1.000], mean observation: 0.111 [-0.340, 0.824], loss: 9.088024, mean_absolute_error: 17.780354, mean_q: 34.749702, mean_eps: 0.171363\n",
      "  8828/50000: episode: 254, duration: 0.202s, episode steps: 77, steps per second: 382, episode reward: 77.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.532 [0.000, 1.000], mean observation: 0.094 [-0.502, 0.858], loss: 9.622244, mean_absolute_error: 17.910230, mean_q: 34.900512, mean_eps: 0.165045\n",
      "  8915/50000: episode: 255, duration: 0.229s, episode steps: 87, steps per second: 381, episode reward: 87.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.117 [-0.434, 1.049], loss: 10.550232, mean_absolute_error: 18.024594, mean_q: 35.031952, mean_eps: 0.157255\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  9027/50000: episode: 256, duration: 0.326s, episode steps: 112, steps per second: 344, episode reward: 112.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.518 [0.000, 1.000], mean observation: 0.079 [-0.578, 0.871], loss: 10.657747, mean_absolute_error: 18.142294, mean_q: 35.290063, mean_eps: 0.147803\n",
      "  9088/50000: episode: 257, duration: 0.171s, episode steps: 61, steps per second: 357, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.541 [0.000, 1.000], mean observation: 0.158 [-0.709, 0.931], loss: 11.987586, mean_absolute_error: 18.025737, mean_q: 35.027315, mean_eps: 0.139585\n",
      "  9177/50000: episode: 258, duration: 0.226s, episode steps: 89, steps per second: 393, episode reward: 89.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: 0.124 [-0.492, 0.883], loss: 8.842292, mean_absolute_error: 18.156958, mean_q: 35.425089, mean_eps: 0.132460\n",
      "  9238/50000: episode: 259, duration: 0.157s, episode steps: 61, steps per second: 387, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.127 [-0.935, 0.312], loss: 10.768501, mean_absolute_error: 18.169332, mean_q: 35.328044, mean_eps: 0.125335\n",
      "  9288/50000: episode: 260, duration: 0.145s, episode steps: 50, steps per second: 344, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.460 [0.000, 1.000], mean observation: -0.146 [-0.817, 0.430], loss: 13.862720, mean_absolute_error: 18.188403, mean_q: 35.099726, mean_eps: 0.120063\n",
      "  9423/50000: episode: 261, duration: 0.457s, episode steps: 135, steps per second: 296, episode reward: 135.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: -0.121 [-0.833, 0.564], loss: 9.847502, mean_absolute_error: 18.236225, mean_q: 35.545485, mean_eps: 0.111275\n",
      "  9623/50000: episode: 262, duration: 0.581s, episode steps: 200, steps per second: 344, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.099 [-0.729, 0.538], loss: 9.595865, mean_absolute_error: 18.258193, mean_q: 35.617101, mean_eps: 0.095363\n",
      "  9689/50000: episode: 263, duration: 0.182s, episode steps: 66, steps per second: 364, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: 0.186 [-0.218, 1.067], loss: 10.164081, mean_absolute_error: 18.430427, mean_q: 35.871458, mean_eps: 0.082728\n",
      "  9764/50000: episode: 264, duration: 0.232s, episode steps: 75, steps per second: 324, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.141 [-0.928, 0.308], loss: 8.637283, mean_absolute_error: 18.416350, mean_q: 35.968788, mean_eps: 0.076030\n",
      "  9964/50000: episode: 265, duration: 0.555s, episode steps: 200, steps per second: 361, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.074 [-0.733, 0.495], loss: 9.341160, mean_absolute_error: 18.642044, mean_q: 36.417628, mean_eps: 0.062968\n",
      " 10019/50000: episode: 266, duration: 0.174s, episode steps: 55, steps per second: 316, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.142 [-0.875, 0.257], loss: 12.516474, mean_absolute_error: 18.924869, mean_q: 36.810234, mean_eps: 0.051150\n",
      " 10106/50000: episode: 267, duration: 0.238s, episode steps: 87, steps per second: 366, episode reward: 87.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.540 [0.000, 1.000], mean observation: 0.160 [-0.240, 1.231], loss: 10.189244, mean_absolute_error: 18.811694, mean_q: 36.592408, mean_eps: 0.050000\n",
      " 10203/50000: episode: 268, duration: 0.245s, episode steps: 97, steps per second: 396, episode reward: 97.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.536 [0.000, 1.000], mean observation: 0.174 [-0.410, 1.299], loss: 8.224365, mean_absolute_error: 18.961053, mean_q: 37.186978, mean_eps: 0.050000\n",
      " 10256/50000: episode: 269, duration: 0.134s, episode steps: 53, steps per second: 396, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.453 [0.000, 1.000], mean observation: -0.153 [-0.932, 0.174], loss: 10.742120, mean_absolute_error: 19.137530, mean_q: 37.340101, mean_eps: 0.050000\n",
      " 10379/50000: episode: 270, duration: 0.322s, episode steps: 123, steps per second: 381, episode reward: 123.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: 0.138 [-0.278, 1.302], loss: 10.654835, mean_absolute_error: 19.128371, mean_q: 37.318946, mean_eps: 0.050000\n",
      " 10485/50000: episode: 271, duration: 0.269s, episode steps: 106, steps per second: 394, episode reward: 106.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: 0.208 [-0.433, 1.313], loss: 9.904488, mean_absolute_error: 19.255468, mean_q: 37.732957, mean_eps: 0.050000\n",
      " 10544/50000: episode: 272, duration: 0.153s, episode steps: 59, steps per second: 387, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: -0.173 [-1.085, 0.408], loss: 10.122718, mean_absolute_error: 19.375401, mean_q: 38.050726, mean_eps: 0.050000\n",
      " 10744/50000: episode: 273, duration: 0.505s, episode steps: 200, steps per second: 396, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.027 [-0.380, 0.426], loss: 11.313540, mean_absolute_error: 19.555435, mean_q: 38.278317, mean_eps: 0.050000\n",
      " 10944/50000: episode: 274, duration: 0.553s, episode steps: 200, steps per second: 362, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: -0.002 [-0.544, 0.643], loss: 9.662965, mean_absolute_error: 19.718555, mean_q: 38.735724, mean_eps: 0.050000\n",
      " 11033/50000: episode: 275, duration: 0.281s, episode steps: 89, steps per second: 317, episode reward: 89.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.171 [-1.078, 0.269], loss: 10.151829, mean_absolute_error: 19.922977, mean_q: 39.161239, mean_eps: 0.050000\n",
      " 11233/50000: episode: 276, duration: 0.599s, episode steps: 200, steps per second: 334, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.018 [-0.333, 0.342], loss: 11.534769, mean_absolute_error: 19.985339, mean_q: 39.164624, mean_eps: 0.050000\n",
      " 11348/50000: episode: 277, duration: 0.346s, episode steps: 115, steps per second: 332, episode reward: 115.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.539 [0.000, 1.000], mean observation: 0.261 [-0.498, 1.680], loss: 9.081060, mean_absolute_error: 20.217939, mean_q: 39.830808, mean_eps: 0.050000\n",
      " 11485/50000: episode: 278, duration: 0.346s, episode steps: 137, steps per second: 396, episode reward: 137.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.237 [-0.451, 1.679], loss: 7.430459, mean_absolute_error: 20.340266, mean_q: 40.204862, mean_eps: 0.050000\n",
      " 11685/50000: episode: 279, duration: 0.503s, episode steps: 200, steps per second: 398, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.012 [-0.642, 0.543], loss: 9.442475, mean_absolute_error: 20.686782, mean_q: 40.903500, mean_eps: 0.050000\n",
      " 11885/50000: episode: 280, duration: 0.501s, episode steps: 200, steps per second: 399, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.074 [-0.544, 0.736], loss: 10.247044, mean_absolute_error: 20.992800, mean_q: 41.513184, mean_eps: 0.050000\n",
      " 12004/50000: episode: 281, duration: 0.292s, episode steps: 119, steps per second: 408, episode reward: 119.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.546 [0.000, 1.000], mean observation: 0.339 [-0.444, 2.241], loss: 9.695423, mean_absolute_error: 21.093493, mean_q: 41.670046, mean_eps: 0.050000\n",
      " 12141/50000: episode: 282, duration: 0.346s, episode steps: 137, steps per second: 396, episode reward: 137.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.258 [-0.388, 1.788], loss: 10.477276, mean_absolute_error: 21.335324, mean_q: 42.153966, mean_eps: 0.050000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 12341/50000: episode: 283, duration: 0.499s, episode steps: 200, steps per second: 401, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: -0.005 [-0.582, 0.463], loss: 11.750365, mean_absolute_error: 21.603866, mean_q: 42.561425, mean_eps: 0.050000\n",
      " 12541/50000: episode: 284, duration: 0.509s, episode steps: 200, steps per second: 393, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.184 [-1.181, 0.407], loss: 10.235331, mean_absolute_error: 21.789592, mean_q: 43.079889, mean_eps: 0.050000\n",
      " 12741/50000: episode: 285, duration: 0.498s, episode steps: 200, steps per second: 402, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.149 [-0.729, 1.233], loss: 10.045539, mean_absolute_error: 22.006873, mean_q: 43.534778, mean_eps: 0.050000\n",
      " 12941/50000: episode: 286, duration: 0.531s, episode steps: 200, steps per second: 376, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.209 [-0.398, 1.609], loss: 9.159248, mean_absolute_error: 22.242639, mean_q: 44.123629, mean_eps: 0.050000\n",
      " 13141/50000: episode: 287, duration: 0.493s, episode steps: 200, steps per second: 406, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.042 [-0.563, 0.351], loss: 10.303520, mean_absolute_error: 22.588581, mean_q: 44.811914, mean_eps: 0.050000\n",
      " 13315/50000: episode: 288, duration: 0.424s, episode steps: 174, steps per second: 410, episode reward: 174.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.395 [-0.491, 2.406], loss: 10.417349, mean_absolute_error: 22.900647, mean_q: 45.443696, mean_eps: 0.050000\n",
      " 13472/50000: episode: 289, duration: 0.386s, episode steps: 157, steps per second: 407, episode reward: 157.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.535 [0.000, 1.000], mean observation: 0.408 [-0.286, 2.413], loss: 10.500751, mean_absolute_error: 23.273204, mean_q: 46.138236, mean_eps: 0.050000\n",
      " 13672/50000: episode: 290, duration: 0.498s, episode steps: 200, steps per second: 402, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.220 [-0.436, 1.544], loss: 11.238124, mean_absolute_error: 23.552341, mean_q: 46.739339, mean_eps: 0.050000\n",
      " 13828/50000: episode: 291, duration: 0.407s, episode steps: 156, steps per second: 384, episode reward: 156.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.437 [-0.472, 2.406], loss: 10.912255, mean_absolute_error: 23.751339, mean_q: 47.106599, mean_eps: 0.050000\n",
      " 14028/50000: episode: 292, duration: 0.521s, episode steps: 200, steps per second: 384, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.266 [-0.543, 1.839], loss: 9.185516, mean_absolute_error: 24.019293, mean_q: 47.816110, mean_eps: 0.050000\n",
      " 14228/50000: episode: 293, duration: 0.602s, episode steps: 200, steps per second: 332, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.173 [-0.497, 1.248], loss: 8.820574, mean_absolute_error: 24.372003, mean_q: 48.569269, mean_eps: 0.050000\n",
      " 14428/50000: episode: 294, duration: 0.655s, episode steps: 200, steps per second: 305, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.207 [-0.433, 1.459], loss: 10.248922, mean_absolute_error: 24.720222, mean_q: 49.231710, mean_eps: 0.050000\n",
      " 14628/50000: episode: 295, duration: 0.636s, episode steps: 200, steps per second: 314, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.318 [-0.484, 2.140], loss: 8.997243, mean_absolute_error: 24.936604, mean_q: 49.626900, mean_eps: 0.050000\n",
      " 14828/50000: episode: 296, duration: 0.566s, episode steps: 200, steps per second: 353, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.344 [-0.370, 2.186], loss: 10.918446, mean_absolute_error: 25.128671, mean_q: 49.981034, mean_eps: 0.050000\n",
      " 15028/50000: episode: 297, duration: 0.673s, episode steps: 200, steps per second: 297, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.373 [-0.538, 2.394], loss: 10.185539, mean_absolute_error: 25.356151, mean_q: 50.504558, mean_eps: 0.050000\n",
      " 15224/50000: episode: 298, duration: 0.574s, episode steps: 196, steps per second: 342, episode reward: 196.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.363 [-0.511, 2.403], loss: 11.485332, mean_absolute_error: 25.825924, mean_q: 51.333013, mean_eps: 0.050000\n",
      " 15407/50000: episode: 299, duration: 0.449s, episode steps: 183, steps per second: 408, episode reward: 183.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.403 [-0.508, 2.418], loss: 9.186994, mean_absolute_error: 26.212516, mean_q: 52.254704, mean_eps: 0.050000\n",
      " 15607/50000: episode: 300, duration: 0.508s, episode steps: 200, steps per second: 394, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.299 [-0.356, 1.904], loss: 8.995635, mean_absolute_error: 26.529018, mean_q: 52.959513, mean_eps: 0.050000\n",
      " 15805/50000: episode: 301, duration: 0.500s, episode steps: 198, steps per second: 396, episode reward: 198.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.380 [-0.672, 2.415], loss: 8.550119, mean_absolute_error: 26.779898, mean_q: 53.436343, mean_eps: 0.050000\n",
      " 16005/50000: episode: 302, duration: 0.497s, episode steps: 200, steps per second: 402, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.325 [-0.641, 1.941], loss: 8.538182, mean_absolute_error: 26.998652, mean_q: 53.970371, mean_eps: 0.050000\n",
      " 16196/50000: episode: 303, duration: 0.490s, episode steps: 191, steps per second: 390, episode reward: 191.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.518 [0.000, 1.000], mean observation: 0.397 [-0.343, 2.405], loss: 7.920513, mean_absolute_error: 27.339818, mean_q: 54.668599, mean_eps: 0.050000\n",
      " 16396/50000: episode: 304, duration: 0.548s, episode steps: 200, steps per second: 365, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.211 [-0.491, 1.390], loss: 9.075549, mean_absolute_error: 27.671862, mean_q: 55.300220, mean_eps: 0.050000\n",
      " 16596/50000: episode: 305, duration: 0.530s, episode steps: 200, steps per second: 377, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.351 [-0.525, 2.259], loss: 8.126568, mean_absolute_error: 27.872001, mean_q: 55.744375, mean_eps: 0.050000\n",
      " 16796/50000: episode: 306, duration: 0.488s, episode steps: 200, steps per second: 410, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.362 [-0.335, 2.285], loss: 8.877586, mean_absolute_error: 28.290834, mean_q: 56.550272, mean_eps: 0.050000\n",
      " 16996/50000: episode: 307, duration: 0.522s, episode steps: 200, steps per second: 383, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.291 [-0.612, 1.811], loss: 9.891007, mean_absolute_error: 28.330823, mean_q: 56.566274, mean_eps: 0.050000\n",
      " 17196/50000: episode: 308, duration: 0.506s, episode steps: 200, steps per second: 395, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.289 [-0.406, 1.816], loss: 8.755799, mean_absolute_error: 28.543703, mean_q: 57.076335, mean_eps: 0.050000\n",
      " 17396/50000: episode: 309, duration: 0.536s, episode steps: 200, steps per second: 373, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.167 [-0.471, 1.058], loss: 8.478922, mean_absolute_error: 28.890219, mean_q: 57.808250, mean_eps: 0.050000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 17596/50000: episode: 310, duration: 0.514s, episode steps: 200, steps per second: 389, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.157 [-0.411, 0.982], loss: 7.453832, mean_absolute_error: 28.966702, mean_q: 57.926329, mean_eps: 0.050000\n",
      " 17796/50000: episode: 311, duration: 0.505s, episode steps: 200, steps per second: 396, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.183 [-0.932, 1.142], loss: 6.969012, mean_absolute_error: 29.277246, mean_q: 58.602483, mean_eps: 0.050000\n",
      " 17996/50000: episode: 312, duration: 0.500s, episode steps: 200, steps per second: 400, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.239 [-0.515, 1.484], loss: 9.558787, mean_absolute_error: 29.447801, mean_q: 58.811481, mean_eps: 0.050000\n",
      " 18196/50000: episode: 313, duration: 0.518s, episode steps: 200, steps per second: 386, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.100 [-0.582, 0.937], loss: 8.329321, mean_absolute_error: 29.909340, mean_q: 59.872901, mean_eps: 0.050000\n",
      " 18396/50000: episode: 314, duration: 0.514s, episode steps: 200, steps per second: 389, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.173 [-0.642, 1.074], loss: 7.352592, mean_absolute_error: 30.115542, mean_q: 60.243182, mean_eps: 0.050000\n",
      " 18596/50000: episode: 315, duration: 0.568s, episode steps: 200, steps per second: 352, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.212 [-0.632, 1.264], loss: 8.404200, mean_absolute_error: 30.216053, mean_q: 60.367988, mean_eps: 0.050000\n",
      " 18796/50000: episode: 316, duration: 0.567s, episode steps: 200, steps per second: 353, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.152 [-0.483, 0.935], loss: 7.645782, mean_absolute_error: 30.470834, mean_q: 60.970374, mean_eps: 0.050000\n",
      " 18996/50000: episode: 317, duration: 0.598s, episode steps: 200, steps per second: 335, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.124 [-0.555, 0.713], loss: 6.608544, mean_absolute_error: 30.603836, mean_q: 61.265083, mean_eps: 0.050000\n",
      " 19196/50000: episode: 318, duration: 0.549s, episode steps: 200, steps per second: 364, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.221 [-0.415, 1.350], loss: 8.952882, mean_absolute_error: 30.902733, mean_q: 61.762409, mean_eps: 0.050000\n",
      " 19396/50000: episode: 319, duration: 0.584s, episode steps: 200, steps per second: 342, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.240 [-0.365, 1.431], loss: 7.208949, mean_absolute_error: 30.806726, mean_q: 61.674773, mean_eps: 0.050000\n",
      " 19596/50000: episode: 320, duration: 0.506s, episode steps: 200, steps per second: 395, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.117 [-0.541, 0.833], loss: 5.853376, mean_absolute_error: 30.798407, mean_q: 61.694035, mean_eps: 0.050000\n",
      " 19796/50000: episode: 321, duration: 0.486s, episode steps: 200, steps per second: 411, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.153 [-0.515, 0.971], loss: 5.871820, mean_absolute_error: 31.026493, mean_q: 62.164115, mean_eps: 0.050000\n",
      " 19996/50000: episode: 322, duration: 0.511s, episode steps: 200, steps per second: 391, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.215 [-0.356, 1.326], loss: 6.663241, mean_absolute_error: 31.455534, mean_q: 63.007530, mean_eps: 0.050000\n",
      " 20196/50000: episode: 323, duration: 0.494s, episode steps: 200, steps per second: 405, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.242 [-0.505, 1.494], loss: 6.011267, mean_absolute_error: 31.699572, mean_q: 63.539446, mean_eps: 0.050000\n",
      " 20396/50000: episode: 324, duration: 0.515s, episode steps: 200, steps per second: 388, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.175 [-0.536, 1.076], loss: 8.810135, mean_absolute_error: 31.891919, mean_q: 63.820081, mean_eps: 0.050000\n",
      " 20596/50000: episode: 325, duration: 0.558s, episode steps: 200, steps per second: 358, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.060 [-0.488, 0.617], loss: 4.883398, mean_absolute_error: 31.992758, mean_q: 64.183605, mean_eps: 0.050000\n",
      " 20796/50000: episode: 326, duration: 0.511s, episode steps: 200, steps per second: 392, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.239 [-0.463, 1.465], loss: 4.987030, mean_absolute_error: 32.253426, mean_q: 64.672160, mean_eps: 0.050000\n",
      " 20996/50000: episode: 327, duration: 0.548s, episode steps: 200, steps per second: 365, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.159 [-0.434, 1.077], loss: 3.304481, mean_absolute_error: 32.208476, mean_q: 64.629207, mean_eps: 0.050000\n",
      " 21196/50000: episode: 328, duration: 0.497s, episode steps: 200, steps per second: 402, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.191 [-0.305, 1.101], loss: 6.288450, mean_absolute_error: 32.509010, mean_q: 65.165838, mean_eps: 0.050000\n",
      " 21396/50000: episode: 329, duration: 0.536s, episode steps: 200, steps per second: 373, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.135 [-0.604, 0.772], loss: 6.525679, mean_absolute_error: 32.404665, mean_q: 64.941938, mean_eps: 0.050000\n",
      " 21596/50000: episode: 330, duration: 0.508s, episode steps: 200, steps per second: 394, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.005 [-0.729, 0.607], loss: 6.845999, mean_absolute_error: 32.411463, mean_q: 64.904061, mean_eps: 0.050000\n",
      " 21796/50000: episode: 331, duration: 0.491s, episode steps: 200, steps per second: 408, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.023 [-0.378, 0.503], loss: 5.763645, mean_absolute_error: 32.281093, mean_q: 64.719512, mean_eps: 0.050000\n",
      " 21996/50000: episode: 332, duration: 0.559s, episode steps: 200, steps per second: 358, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.114 [-0.620, 0.785], loss: 4.694316, mean_absolute_error: 32.509334, mean_q: 65.156372, mean_eps: 0.050000\n",
      " 22196/50000: episode: 333, duration: 0.514s, episode steps: 200, steps per second: 389, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.233 [-0.613, 1.397], loss: 4.726011, mean_absolute_error: 32.558517, mean_q: 65.225786, mean_eps: 0.050000\n",
      " 22396/50000: episode: 334, duration: 0.484s, episode steps: 200, steps per second: 413, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.043 [-0.766, 0.566], loss: 4.478145, mean_absolute_error: 32.372996, mean_q: 64.915949, mean_eps: 0.050000\n",
      " 22596/50000: episode: 335, duration: 0.510s, episode steps: 200, steps per second: 392, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.173 [-0.521, 1.015], loss: 5.762723, mean_absolute_error: 32.276133, mean_q: 64.613608, mean_eps: 0.050000\n",
      " 22796/50000: episode: 336, duration: 0.499s, episode steps: 200, steps per second: 401, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.077 [-0.466, 0.537], loss: 4.310955, mean_absolute_error: 32.434296, mean_q: 64.964161, mean_eps: 0.050000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 22996/50000: episode: 337, duration: 0.500s, episode steps: 200, steps per second: 400, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.048 [-0.374, 0.614], loss: 6.506877, mean_absolute_error: 32.527287, mean_q: 65.088384, mean_eps: 0.050000\n",
      " 23196/50000: episode: 338, duration: 0.492s, episode steps: 200, steps per second: 406, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.152 [-0.477, 0.906], loss: 6.393116, mean_absolute_error: 32.582568, mean_q: 65.166283, mean_eps: 0.050000\n",
      " 23396/50000: episode: 339, duration: 0.495s, episode steps: 200, steps per second: 404, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.032 [-0.378, 0.448], loss: 4.681974, mean_absolute_error: 32.800659, mean_q: 65.696759, mean_eps: 0.050000\n",
      " 23596/50000: episode: 340, duration: 0.482s, episode steps: 200, steps per second: 415, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.115 [-0.563, 0.706], loss: 4.724057, mean_absolute_error: 32.981738, mean_q: 66.023109, mean_eps: 0.050000\n",
      " 23796/50000: episode: 341, duration: 0.484s, episode steps: 200, steps per second: 413, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.185 [-0.583, 1.077], loss: 6.802318, mean_absolute_error: 33.201565, mean_q: 66.401848, mean_eps: 0.050000\n",
      " 23996/50000: episode: 342, duration: 0.484s, episode steps: 200, steps per second: 413, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.134 [-0.471, 0.801], loss: 4.129309, mean_absolute_error: 33.335175, mean_q: 66.737240, mean_eps: 0.050000\n",
      " 24196/50000: episode: 343, duration: 0.490s, episode steps: 200, steps per second: 408, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.184 [-0.547, 1.138], loss: 6.707824, mean_absolute_error: 33.397427, mean_q: 66.751864, mean_eps: 0.050000\n",
      " 24396/50000: episode: 344, duration: 0.503s, episode steps: 200, steps per second: 398, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.146 [-0.563, 0.831], loss: 3.501476, mean_absolute_error: 33.598426, mean_q: 67.315987, mean_eps: 0.050000\n",
      " 24596/50000: episode: 345, duration: 0.497s, episode steps: 200, steps per second: 402, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.117 [-0.609, 0.754], loss: 7.129638, mean_absolute_error: 33.734007, mean_q: 67.449778, mean_eps: 0.050000\n",
      " 24796/50000: episode: 346, duration: 0.493s, episode steps: 200, steps per second: 405, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.105 [-0.495, 0.622], loss: 5.187212, mean_absolute_error: 34.058255, mean_q: 68.139631, mean_eps: 0.050000\n",
      " 24996/50000: episode: 347, duration: 0.503s, episode steps: 200, steps per second: 397, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.136 [-0.354, 0.798], loss: 6.105076, mean_absolute_error: 33.973816, mean_q: 67.912107, mean_eps: 0.050000\n",
      " 25196/50000: episode: 348, duration: 0.509s, episode steps: 200, steps per second: 393, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.085 [-0.533, 0.528], loss: 4.932030, mean_absolute_error: 34.138025, mean_q: 68.278934, mean_eps: 0.050000\n",
      " 25396/50000: episode: 349, duration: 0.495s, episode steps: 200, steps per second: 404, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.078 [-0.375, 0.590], loss: 6.554345, mean_absolute_error: 34.579954, mean_q: 69.072054, mean_eps: 0.050000\n",
      " 25596/50000: episode: 350, duration: 0.495s, episode steps: 200, steps per second: 404, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.076 [-0.582, 0.616], loss: 5.458342, mean_absolute_error: 34.624176, mean_q: 69.213740, mean_eps: 0.050000\n",
      " 25796/50000: episode: 351, duration: 0.493s, episode steps: 200, steps per second: 405, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.146 [-0.403, 0.833], loss: 5.722373, mean_absolute_error: 34.700294, mean_q: 69.337121, mean_eps: 0.050000\n",
      " 25996/50000: episode: 352, duration: 0.522s, episode steps: 200, steps per second: 383, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.102 [-0.790, 0.780], loss: 6.162664, mean_absolute_error: 34.832950, mean_q: 69.539187, mean_eps: 0.050000\n",
      " 26196/50000: episode: 353, duration: 0.498s, episode steps: 200, steps per second: 402, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.161 [-0.465, 0.923], loss: 5.750757, mean_absolute_error: 35.064488, mean_q: 70.049642, mean_eps: 0.050000\n",
      " 26396/50000: episode: 354, duration: 0.502s, episode steps: 200, steps per second: 399, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.044 [-0.536, 0.559], loss: 6.321521, mean_absolute_error: 34.974329, mean_q: 69.836857, mean_eps: 0.050000\n",
      " 26596/50000: episode: 355, duration: 0.494s, episode steps: 200, steps per second: 405, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.087 [-0.738, 0.719], loss: 6.668683, mean_absolute_error: 35.400478, mean_q: 70.724193, mean_eps: 0.050000\n",
      " 26796/50000: episode: 356, duration: 0.517s, episode steps: 200, steps per second: 387, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.093 [-0.615, 0.602], loss: 5.585361, mean_absolute_error: 35.644410, mean_q: 71.227352, mean_eps: 0.050000\n",
      " 26996/50000: episode: 357, duration: 0.501s, episode steps: 200, steps per second: 400, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.152 [-0.407, 0.882], loss: 8.928363, mean_absolute_error: 35.470959, mean_q: 70.710588, mean_eps: 0.050000\n",
      " 27196/50000: episode: 358, duration: 0.500s, episode steps: 200, steps per second: 400, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.154 [-0.873, 0.942], loss: 6.057188, mean_absolute_error: 35.552764, mean_q: 70.995979, mean_eps: 0.050000\n",
      " 27396/50000: episode: 359, duration: 0.495s, episode steps: 200, steps per second: 404, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.138 [-0.521, 0.880], loss: 5.315107, mean_absolute_error: 35.367148, mean_q: 70.605434, mean_eps: 0.050000\n",
      " 27596/50000: episode: 360, duration: 0.486s, episode steps: 200, steps per second: 412, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.108 [-0.602, 0.651], loss: 4.312232, mean_absolute_error: 35.497396, mean_q: 70.941060, mean_eps: 0.050000\n",
      " 27796/50000: episode: 361, duration: 0.484s, episode steps: 200, steps per second: 413, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.132 [-0.377, 0.791], loss: 5.682147, mean_absolute_error: 35.286333, mean_q: 70.476649, mean_eps: 0.050000\n",
      " 27996/50000: episode: 362, duration: 0.483s, episode steps: 200, steps per second: 414, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.161 [-0.422, 0.972], loss: 4.545211, mean_absolute_error: 35.472088, mean_q: 70.873810, mean_eps: 0.050000\n",
      " 28196/50000: episode: 363, duration: 0.505s, episode steps: 200, steps per second: 396, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.087 [-0.432, 0.599], loss: 5.975185, mean_absolute_error: 35.443177, mean_q: 70.725720, mean_eps: 0.050000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 28396/50000: episode: 364, duration: 0.606s, episode steps: 200, steps per second: 330, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.106 [-0.405, 0.663], loss: 3.715341, mean_absolute_error: 35.213592, mean_q: 70.384743, mean_eps: 0.050000\n",
      " 28596/50000: episode: 365, duration: 0.488s, episode steps: 200, steps per second: 410, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.140 [-0.448, 0.820], loss: 5.648756, mean_absolute_error: 35.366050, mean_q: 70.635232, mean_eps: 0.050000\n",
      " 28796/50000: episode: 366, duration: 0.495s, episode steps: 200, steps per second: 404, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.066 [-0.435, 0.522], loss: 4.394115, mean_absolute_error: 35.183986, mean_q: 70.309164, mean_eps: 0.050000\n",
      " 28996/50000: episode: 367, duration: 0.483s, episode steps: 200, steps per second: 414, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.008 [-0.533, 0.636], loss: 4.105759, mean_absolute_error: 35.642354, mean_q: 71.277451, mean_eps: 0.050000\n",
      " 29196/50000: episode: 368, duration: 0.512s, episode steps: 200, steps per second: 391, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.067 [-0.665, 0.590], loss: 5.631299, mean_absolute_error: 35.663525, mean_q: 71.201230, mean_eps: 0.050000\n",
      " 29396/50000: episode: 369, duration: 0.517s, episode steps: 200, steps per second: 387, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.071 [-0.637, 0.535], loss: 5.922783, mean_absolute_error: 36.144300, mean_q: 72.151052, mean_eps: 0.050000\n",
      " 29596/50000: episode: 370, duration: 0.492s, episode steps: 200, steps per second: 407, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.037 [-0.576, 0.586], loss: 8.513526, mean_absolute_error: 36.375021, mean_q: 72.429671, mean_eps: 0.050000\n",
      " 29796/50000: episode: 371, duration: 0.500s, episode steps: 200, steps per second: 400, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.102 [-0.613, 0.579], loss: 3.167825, mean_absolute_error: 36.205253, mean_q: 72.374396, mean_eps: 0.050000\n",
      " 29996/50000: episode: 372, duration: 0.600s, episode steps: 200, steps per second: 334, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.148 [-0.574, 0.870], loss: 6.593191, mean_absolute_error: 36.407680, mean_q: 72.610880, mean_eps: 0.050000\n",
      " 30196/50000: episode: 373, duration: 0.519s, episode steps: 200, steps per second: 386, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.117 [-0.513, 0.667], loss: 6.433744, mean_absolute_error: 36.428630, mean_q: 72.631295, mean_eps: 0.050000\n",
      " 30396/50000: episode: 374, duration: 0.503s, episode steps: 200, steps per second: 397, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.063 [-0.563, 0.579], loss: 7.058135, mean_absolute_error: 36.721863, mean_q: 73.148280, mean_eps: 0.050000\n",
      " 30596/50000: episode: 375, duration: 0.496s, episode steps: 200, steps per second: 403, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.085 [-0.814, 0.855], loss: 6.164729, mean_absolute_error: 36.642265, mean_q: 73.069336, mean_eps: 0.050000\n",
      " 30796/50000: episode: 376, duration: 0.522s, episode steps: 200, steps per second: 383, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.107 [-0.445, 0.634], loss: 6.537408, mean_absolute_error: 36.467276, mean_q: 72.728523, mean_eps: 0.050000\n",
      " 30996/50000: episode: 377, duration: 0.520s, episode steps: 200, steps per second: 385, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.117 [-0.582, 0.720], loss: 4.853655, mean_absolute_error: 36.682114, mean_q: 73.189209, mean_eps: 0.050000\n",
      " 31196/50000: episode: 378, duration: 0.513s, episode steps: 200, steps per second: 390, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.122 [-0.538, 0.802], loss: 5.971243, mean_absolute_error: 36.890001, mean_q: 73.543656, mean_eps: 0.050000\n",
      " 31396/50000: episode: 379, duration: 0.539s, episode steps: 200, steps per second: 371, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.109 [-0.619, 0.785], loss: 4.445756, mean_absolute_error: 36.807477, mean_q: 73.483706, mean_eps: 0.050000\n",
      " 31596/50000: episode: 380, duration: 0.496s, episode steps: 200, steps per second: 403, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.076 [-0.439, 0.596], loss: 5.944727, mean_absolute_error: 36.506082, mean_q: 72.802211, mean_eps: 0.050000\n",
      " 31796/50000: episode: 381, duration: 0.510s, episode steps: 200, steps per second: 392, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.046 [-0.634, 0.841], loss: 4.879225, mean_absolute_error: 36.481398, mean_q: 72.848840, mean_eps: 0.050000\n",
      " 31996/50000: episode: 382, duration: 0.482s, episode steps: 200, steps per second: 415, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.050 [-0.526, 0.421], loss: 5.377162, mean_absolute_error: 36.517860, mean_q: 72.909675, mean_eps: 0.050000\n",
      " 32196/50000: episode: 383, duration: 0.503s, episode steps: 200, steps per second: 398, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.078 [-0.623, 0.809], loss: 4.739153, mean_absolute_error: 36.582624, mean_q: 72.956426, mean_eps: 0.050000\n",
      " 32396/50000: episode: 384, duration: 0.504s, episode steps: 200, steps per second: 397, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.079 [-0.423, 0.478], loss: 3.370788, mean_absolute_error: 36.550338, mean_q: 73.027637, mean_eps: 0.050000\n",
      " 32596/50000: episode: 385, duration: 0.499s, episode steps: 200, steps per second: 401, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.069 [-0.573, 0.840], loss: 5.604130, mean_absolute_error: 36.582047, mean_q: 72.940937, mean_eps: 0.050000\n",
      " 32796/50000: episode: 386, duration: 0.477s, episode steps: 200, steps per second: 419, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.049 [-0.458, 0.464], loss: 6.441130, mean_absolute_error: 36.511097, mean_q: 72.841147, mean_eps: 0.050000\n",
      " 32996/50000: episode: 387, duration: 0.479s, episode steps: 200, steps per second: 417, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.053 [-0.618, 0.645], loss: 7.458402, mean_absolute_error: 36.436875, mean_q: 72.594161, mean_eps: 0.050000\n",
      " 33196/50000: episode: 388, duration: 0.476s, episode steps: 200, steps per second: 420, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.066 [-0.414, 0.446], loss: 5.899740, mean_absolute_error: 36.422924, mean_q: 72.685479, mean_eps: 0.050000\n",
      " 33396/50000: episode: 389, duration: 0.479s, episode steps: 200, steps per second: 418, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.069 [-0.485, 0.539], loss: 4.651108, mean_absolute_error: 36.253957, mean_q: 72.365120, mean_eps: 0.050000\n",
      " 33596/50000: episode: 390, duration: 0.477s, episode steps: 200, steps per second: 419, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.078 [-0.517, 0.610], loss: 5.118482, mean_absolute_error: 36.088423, mean_q: 71.989687, mean_eps: 0.050000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 33796/50000: episode: 391, duration: 0.478s, episode steps: 200, steps per second: 419, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.031 [-0.702, 0.708], loss: 6.813499, mean_absolute_error: 36.207569, mean_q: 72.179127, mean_eps: 0.050000\n",
      " 33996/50000: episode: 392, duration: 0.477s, episode steps: 200, steps per second: 420, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.046 [-0.529, 0.537], loss: 5.977262, mean_absolute_error: 36.363814, mean_q: 72.508857, mean_eps: 0.050000\n",
      " 34196/50000: episode: 393, duration: 0.515s, episode steps: 200, steps per second: 388, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.059 [-0.510, 0.657], loss: 5.716766, mean_absolute_error: 36.318783, mean_q: 72.423394, mean_eps: 0.050000\n",
      " 34396/50000: episode: 394, duration: 0.520s, episode steps: 200, steps per second: 384, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.042 [-0.569, 0.596], loss: 5.358157, mean_absolute_error: 36.279870, mean_q: 72.379331, mean_eps: 0.050000\n",
      " 34596/50000: episode: 395, duration: 0.486s, episode steps: 200, steps per second: 411, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.048 [-0.528, 0.398], loss: 8.096853, mean_absolute_error: 36.324109, mean_q: 72.342319, mean_eps: 0.050000\n",
      " 34796/50000: episode: 396, duration: 0.493s, episode steps: 200, steps per second: 405, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.127 [-0.891, 0.727], loss: 5.754017, mean_absolute_error: 36.472452, mean_q: 72.900055, mean_eps: 0.050000\n",
      " 34996/50000: episode: 397, duration: 0.495s, episode steps: 200, steps per second: 404, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.003 [-0.371, 0.594], loss: 8.293214, mean_absolute_error: 36.384268, mean_q: 72.473358, mean_eps: 0.050000\n",
      " 35196/50000: episode: 398, duration: 0.487s, episode steps: 200, steps per second: 411, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.044 [-0.455, 0.447], loss: 8.420025, mean_absolute_error: 36.526853, mean_q: 72.760862, mean_eps: 0.050000\n",
      " 35396/50000: episode: 399, duration: 0.491s, episode steps: 200, steps per second: 407, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.015 [-0.666, 0.714], loss: 5.519784, mean_absolute_error: 36.756671, mean_q: 73.365425, mean_eps: 0.050000\n",
      " 35596/50000: episode: 400, duration: 0.490s, episode steps: 200, steps per second: 408, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.028 [-1.045, 0.788], loss: 10.550947, mean_absolute_error: 36.592689, mean_q: 72.806944, mean_eps: 0.050000\n",
      " 35796/50000: episode: 401, duration: 0.487s, episode steps: 200, steps per second: 410, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: -0.028 [-0.790, 0.604], loss: 6.628557, mean_absolute_error: 36.986350, mean_q: 73.756724, mean_eps: 0.050000\n",
      " 35996/50000: episode: 402, duration: 0.482s, episode steps: 200, steps per second: 415, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.008 [-0.678, 0.596], loss: 7.992953, mean_absolute_error: 36.856060, mean_q: 73.551468, mean_eps: 0.050000\n",
      " 36196/50000: episode: 403, duration: 0.496s, episode steps: 200, steps per second: 403, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.004 [-0.496, 0.412], loss: 5.236543, mean_absolute_error: 37.251829, mean_q: 74.399751, mean_eps: 0.050000\n",
      " 36396/50000: episode: 404, duration: 0.499s, episode steps: 200, steps per second: 401, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.017 [-0.445, 0.554], loss: 3.491984, mean_absolute_error: 37.225813, mean_q: 74.388563, mean_eps: 0.050000\n",
      " 36596/50000: episode: 405, duration: 0.552s, episode steps: 200, steps per second: 362, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.082 [-0.597, 0.606], loss: 7.134560, mean_absolute_error: 37.069642, mean_q: 73.889056, mean_eps: 0.050000\n",
      " 36796/50000: episode: 406, duration: 0.504s, episode steps: 200, steps per second: 397, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.038 [-0.830, 0.630], loss: 10.197616, mean_absolute_error: 37.154218, mean_q: 74.016796, mean_eps: 0.050000\n",
      " 36996/50000: episode: 407, duration: 0.500s, episode steps: 200, steps per second: 400, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.117 [-0.443, 0.786], loss: 12.079643, mean_absolute_error: 37.015894, mean_q: 73.574577, mean_eps: 0.050000\n",
      " 37196/50000: episode: 408, duration: 0.507s, episode steps: 200, steps per second: 394, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.082 [-0.620, 0.846], loss: 8.787003, mean_absolute_error: 37.553239, mean_q: 74.755162, mean_eps: 0.050000\n",
      " 37396/50000: episode: 409, duration: 0.487s, episode steps: 200, steps per second: 411, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.091 [-0.542, 0.634], loss: 13.140661, mean_absolute_error: 37.362955, mean_q: 74.213645, mean_eps: 0.050000\n",
      " 37596/50000: episode: 410, duration: 0.485s, episode steps: 200, steps per second: 412, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.061 [-0.519, 0.637], loss: 9.492280, mean_absolute_error: 37.020362, mean_q: 73.722427, mean_eps: 0.050000\n",
      " 37796/50000: episode: 411, duration: 0.490s, episode steps: 200, steps per second: 408, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.052 [-0.568, 0.864], loss: 6.290337, mean_absolute_error: 36.966552, mean_q: 73.669365, mean_eps: 0.050000\n",
      " 37996/50000: episode: 412, duration: 0.501s, episode steps: 200, steps per second: 399, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.143 [-0.759, 0.933], loss: 7.209178, mean_absolute_error: 37.265641, mean_q: 74.238058, mean_eps: 0.050000\n",
      " 38196/50000: episode: 413, duration: 0.497s, episode steps: 200, steps per second: 402, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.210 [-0.577, 1.348], loss: 6.174329, mean_absolute_error: 37.244667, mean_q: 74.258030, mean_eps: 0.050000\n",
      " 38396/50000: episode: 414, duration: 0.506s, episode steps: 200, steps per second: 395, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.140 [-0.535, 0.899], loss: 6.561572, mean_absolute_error: 37.125609, mean_q: 73.860425, mean_eps: 0.050000\n",
      " 38596/50000: episode: 415, duration: 0.597s, episode steps: 200, steps per second: 335, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.086 [-0.407, 0.552], loss: 6.323231, mean_absolute_error: 37.129782, mean_q: 73.739789, mean_eps: 0.050000\n",
      " 38796/50000: episode: 416, duration: 0.509s, episode steps: 200, steps per second: 393, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.024 [-0.793, 1.158], loss: 9.037282, mean_absolute_error: 37.060796, mean_q: 73.467526, mean_eps: 0.050000\n",
      " 38996/50000: episode: 417, duration: 0.490s, episode steps: 200, steps per second: 408, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.040 [-0.624, 0.544], loss: 9.806819, mean_absolute_error: 36.871514, mean_q: 73.079489, mean_eps: 0.050000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 39196/50000: episode: 418, duration: 0.495s, episode steps: 200, steps per second: 404, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.007 [-0.583, 0.603], loss: 10.891399, mean_absolute_error: 36.829249, mean_q: 72.991313, mean_eps: 0.050000\n",
      " 39396/50000: episode: 419, duration: 0.488s, episode steps: 200, steps per second: 410, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.023 [-0.554, 0.764], loss: 12.868482, mean_absolute_error: 37.123263, mean_q: 73.613216, mean_eps: 0.050000\n",
      " 39596/50000: episode: 420, duration: 0.487s, episode steps: 200, steps per second: 411, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.008 [-0.754, 0.670], loss: 11.266116, mean_absolute_error: 37.002506, mean_q: 73.570301, mean_eps: 0.050000\n",
      " 39796/50000: episode: 421, duration: 0.509s, episode steps: 200, steps per second: 393, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.039 [-0.882, 0.608], loss: 7.434605, mean_absolute_error: 37.396734, mean_q: 74.407103, mean_eps: 0.050000\n",
      " 39996/50000: episode: 422, duration: 0.513s, episode steps: 200, steps per second: 390, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.017 [-0.867, 0.611], loss: 8.783165, mean_absolute_error: 37.747839, mean_q: 75.060580, mean_eps: 0.050000\n",
      " 40196/50000: episode: 423, duration: 0.510s, episode steps: 200, steps per second: 392, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.044 [-0.708, 0.573], loss: 9.395033, mean_absolute_error: 38.064769, mean_q: 75.744702, mean_eps: 0.050000\n",
      " 40396/50000: episode: 424, duration: 0.492s, episode steps: 200, steps per second: 407, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.005 [-0.644, 0.473], loss: 11.162417, mean_absolute_error: 38.332891, mean_q: 76.087432, mean_eps: 0.050000\n",
      " 40596/50000: episode: 425, duration: 0.491s, episode steps: 200, steps per second: 407, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.065 [-0.450, 0.469], loss: 14.690242, mean_absolute_error: 38.194042, mean_q: 75.614846, mean_eps: 0.050000\n",
      " 40796/50000: episode: 426, duration: 0.501s, episode steps: 200, steps per second: 400, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.063 [-0.554, 0.839], loss: 10.164442, mean_absolute_error: 38.481844, mean_q: 76.331147, mean_eps: 0.050000\n",
      " 40996/50000: episode: 427, duration: 0.624s, episode steps: 200, steps per second: 321, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.065 [-0.516, 0.548], loss: 9.736168, mean_absolute_error: 38.423826, mean_q: 76.316436, mean_eps: 0.050000\n",
      " 41196/50000: episode: 428, duration: 0.493s, episode steps: 200, steps per second: 406, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.045 [-0.756, 1.169], loss: 12.701442, mean_absolute_error: 38.643336, mean_q: 76.618605, mean_eps: 0.050000\n",
      " 41396/50000: episode: 429, duration: 0.504s, episode steps: 200, steps per second: 397, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.070 [-0.652, 0.847], loss: 8.112202, mean_absolute_error: 38.452882, mean_q: 76.378689, mean_eps: 0.050000\n",
      " 41596/50000: episode: 430, duration: 0.542s, episode steps: 200, steps per second: 369, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.105 [-0.594, 0.861], loss: 9.270390, mean_absolute_error: 38.432599, mean_q: 76.330348, mean_eps: 0.050000\n",
      " 41796/50000: episode: 431, duration: 0.534s, episode steps: 200, steps per second: 374, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.158 [-0.645, 0.926], loss: 11.107369, mean_absolute_error: 38.295426, mean_q: 75.838608, mean_eps: 0.050000\n",
      " 41996/50000: episode: 432, duration: 0.515s, episode steps: 200, steps per second: 388, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.135 [-0.489, 0.811], loss: 6.086729, mean_absolute_error: 38.264958, mean_q: 76.184326, mean_eps: 0.050000\n",
      " 42196/50000: episode: 433, duration: 0.504s, episode steps: 200, steps per second: 397, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.113 [-0.610, 0.857], loss: 11.326068, mean_absolute_error: 37.783776, mean_q: 74.831018, mean_eps: 0.050000\n",
      " 42396/50000: episode: 434, duration: 0.480s, episode steps: 200, steps per second: 417, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.047 [-0.795, 1.122], loss: 13.688425, mean_absolute_error: 37.952241, mean_q: 75.221802, mean_eps: 0.050000\n",
      " 42596/50000: episode: 435, duration: 0.492s, episode steps: 200, steps per second: 407, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.052 [-0.798, 0.842], loss: 8.085511, mean_absolute_error: 37.500046, mean_q: 74.451980, mean_eps: 0.050000\n",
      " 42796/50000: episode: 436, duration: 0.478s, episode steps: 200, steps per second: 418, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.016 [-0.555, 0.596], loss: 12.074110, mean_absolute_error: 37.452082, mean_q: 74.287066, mean_eps: 0.050000\n",
      " 42996/50000: episode: 437, duration: 0.486s, episode steps: 200, steps per second: 412, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.004 [-0.609, 0.558], loss: 14.579933, mean_absolute_error: 37.333092, mean_q: 73.918063, mean_eps: 0.050000\n",
      " 43196/50000: episode: 438, duration: 0.481s, episode steps: 200, steps per second: 416, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.023 [-0.644, 0.655], loss: 10.091236, mean_absolute_error: 37.473677, mean_q: 74.470529, mean_eps: 0.050000\n",
      " 43396/50000: episode: 439, duration: 0.498s, episode steps: 200, steps per second: 402, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.004 [-0.636, 0.518], loss: 15.801060, mean_absolute_error: 37.839976, mean_q: 75.019044, mean_eps: 0.050000\n",
      " 43596/50000: episode: 440, duration: 0.532s, episode steps: 200, steps per second: 376, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.027 [-0.537, 0.679], loss: 10.126497, mean_absolute_error: 37.669009, mean_q: 74.794502, mean_eps: 0.050000\n",
      " 43796/50000: episode: 441, duration: 0.498s, episode steps: 200, steps per second: 401, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.001 [-0.843, 0.785], loss: 8.739990, mean_absolute_error: 37.583211, mean_q: 74.681125, mean_eps: 0.050000\n",
      " 43996/50000: episode: 442, duration: 0.518s, episode steps: 200, steps per second: 386, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.002 [-0.543, 0.613], loss: 9.315955, mean_absolute_error: 37.927914, mean_q: 75.472602, mean_eps: 0.050000\n",
      " 44196/50000: episode: 443, duration: 0.514s, episode steps: 200, steps per second: 389, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.009 [-0.544, 0.615], loss: 8.557681, mean_absolute_error: 37.824007, mean_q: 75.234717, mean_eps: 0.050000\n",
      " 44396/50000: episode: 444, duration: 0.503s, episode steps: 200, steps per second: 397, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.003 [-0.618, 0.857], loss: 12.416815, mean_absolute_error: 38.187368, mean_q: 75.813613, mean_eps: 0.050000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 44596/50000: episode: 445, duration: 0.514s, episode steps: 200, steps per second: 389, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.045 [-1.107, 1.168], loss: 11.245718, mean_absolute_error: 38.247369, mean_q: 76.043513, mean_eps: 0.050000\n",
      " 44796/50000: episode: 446, duration: 0.500s, episode steps: 200, steps per second: 400, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.017 [-1.140, 1.181], loss: 13.377765, mean_absolute_error: 38.428352, mean_q: 76.206466, mean_eps: 0.050000\n",
      " 44996/50000: episode: 447, duration: 0.513s, episode steps: 200, steps per second: 390, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.060 [-0.447, 0.439], loss: 10.168802, mean_absolute_error: 38.383181, mean_q: 76.327618, mean_eps: 0.050000\n",
      " 45196/50000: episode: 448, duration: 0.625s, episode steps: 200, steps per second: 320, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.073 [-0.527, 0.663], loss: 13.877704, mean_absolute_error: 38.323877, mean_q: 76.000553, mean_eps: 0.050000\n",
      " 45396/50000: episode: 449, duration: 0.522s, episode steps: 200, steps per second: 383, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.026 [-0.567, 0.606], loss: 11.864726, mean_absolute_error: 37.889599, mean_q: 75.179179, mean_eps: 0.050000\n",
      " 45596/50000: episode: 450, duration: 0.500s, episode steps: 200, steps per second: 400, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.032 [-0.693, 0.717], loss: 12.472520, mean_absolute_error: 37.757820, mean_q: 75.010126, mean_eps: 0.050000\n",
      " 45796/50000: episode: 451, duration: 0.510s, episode steps: 200, steps per second: 392, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.030 [-0.526, 0.448], loss: 9.989004, mean_absolute_error: 37.817411, mean_q: 75.176511, mean_eps: 0.050000\n",
      " 45996/50000: episode: 452, duration: 0.485s, episode steps: 200, steps per second: 413, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.017 [-0.560, 0.877], loss: 8.170634, mean_absolute_error: 37.859721, mean_q: 75.376007, mean_eps: 0.050000\n",
      " 46168/50000: episode: 453, duration: 0.418s, episode steps: 172, steps per second: 411, episode reward: 172.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: -0.028 [-2.314, 2.697], loss: 12.325497, mean_absolute_error: 37.840693, mean_q: 75.195310, mean_eps: 0.050000\n",
      " 46269/50000: episode: 454, duration: 0.242s, episode steps: 101, steps per second: 418, episode reward: 101.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: -0.040 [-2.536, 2.164], loss: 10.045969, mean_absolute_error: 38.006665, mean_q: 75.582278, mean_eps: 0.050000\n",
      " 46307/50000: episode: 455, duration: 0.098s, episode steps: 38, steps per second: 388, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.579 [0.000, 1.000], mean observation: -0.065 [-2.378, 1.922], loss: 16.047230, mean_absolute_error: 38.050979, mean_q: 75.783748, mean_eps: 0.050000\n",
      " 46355/50000: episode: 456, duration: 0.132s, episode steps: 48, steps per second: 364, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: -0.044 [-2.103, 2.439], loss: 18.237062, mean_absolute_error: 38.294167, mean_q: 75.934145, mean_eps: 0.050000\n",
      " 46508/50000: episode: 457, duration: 0.397s, episode steps: 153, steps per second: 385, episode reward: 153.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.043 [-2.469, 2.754], loss: 11.120730, mean_absolute_error: 38.220127, mean_q: 76.079130, mean_eps: 0.050000\n",
      " 46537/50000: episode: 458, duration: 0.070s, episode steps: 29, steps per second: 412, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.448 [0.000, 1.000], mean observation: -0.040 [-1.954, 2.294], loss: 16.312300, mean_absolute_error: 38.101038, mean_q: 75.735329, mean_eps: 0.050000\n",
      " 46574/50000: episode: 459, duration: 0.091s, episode steps: 37, steps per second: 404, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.568 [0.000, 1.000], mean observation: -0.054 [-2.384, 1.531], loss: 10.969540, mean_absolute_error: 38.257723, mean_q: 76.209173, mean_eps: 0.050000\n",
      " 46584/50000: episode: 460, duration: 0.028s, episode steps: 10, steps per second: 362, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: -0.119 [-2.035, 1.361], loss: 22.889084, mean_absolute_error: 38.610190, mean_q: 76.408240, mean_eps: 0.050000\n",
      " 46595/50000: episode: 461, duration: 0.032s, episode steps: 11, steps per second: 343, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.111 [-2.044, 1.400], loss: 7.525969, mean_absolute_error: 38.696293, mean_q: 77.079131, mean_eps: 0.050000\n",
      " 46608/50000: episode: 462, duration: 0.040s, episode steps: 13, steps per second: 322, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.092 [-2.050, 1.402], loss: 30.316404, mean_absolute_error: 38.571339, mean_q: 76.384191, mean_eps: 0.050000\n",
      " 46659/50000: episode: 463, duration: 0.148s, episode steps: 51, steps per second: 345, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.451 [0.000, 1.000], mean observation: -0.030 [-2.066, 2.394], loss: 11.880653, mean_absolute_error: 38.637955, mean_q: 76.723997, mean_eps: 0.050000\n",
      " 46729/50000: episode: 464, duration: 0.181s, episode steps: 70, steps per second: 386, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: -0.022 [-2.221, 1.346], loss: 20.978601, mean_absolute_error: 38.626637, mean_q: 76.357739, mean_eps: 0.050000\n",
      " 46738/50000: episode: 465, duration: 0.026s, episode steps: 9, steps per second: 340, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.148 [-1.871, 1.134], loss: 26.640206, mean_absolute_error: 38.641232, mean_q: 76.305551, mean_eps: 0.050000\n",
      " 46748/50000: episode: 466, duration: 0.029s, episode steps: 10, steps per second: 342, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.132 [-2.682, 1.751], loss: 18.757893, mean_absolute_error: 38.104601, mean_q: 75.521934, mean_eps: 0.050000\n",
      " 46762/50000: episode: 467, duration: 0.041s, episode steps: 14, steps per second: 343, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.786 [0.000, 1.000], mean observation: -0.105 [-2.698, 1.761], loss: 15.109446, mean_absolute_error: 38.338620, mean_q: 76.360156, mean_eps: 0.050000\n",
      " 46771/50000: episode: 468, duration: 0.026s, episode steps: 9, steps per second: 350, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.150 [-2.186, 1.325], loss: 0.174634, mean_absolute_error: 38.309416, mean_q: 76.822288, mean_eps: 0.050000\n",
      " 46781/50000: episode: 469, duration: 0.029s, episode steps: 10, steps per second: 345, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.143 [-2.710, 1.767], loss: 0.708471, mean_absolute_error: 38.004559, mean_q: 76.222412, mean_eps: 0.050000\n",
      " 46791/50000: episode: 470, duration: 0.028s, episode steps: 10, steps per second: 354, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.130 [-2.394, 1.536], loss: 26.688246, mean_absolute_error: 38.881918, mean_q: 77.180334, mean_eps: 0.050000\n",
      " 46801/50000: episode: 471, duration: 0.030s, episode steps: 10, steps per second: 335, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.161 [-2.769, 1.733], loss: 3.437391, mean_absolute_error: 38.289735, mean_q: 76.440028, mean_eps: 0.050000\n",
      " 46810/50000: episode: 472, duration: 0.025s, episode steps: 9, steps per second: 366, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.138 [-2.163, 1.405], loss: 29.528157, mean_absolute_error: 39.228299, mean_q: 77.573508, mean_eps: 0.050000\n",
      " 46821/50000: episode: 473, duration: 0.028s, episode steps: 11, steps per second: 390, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.132 [-2.298, 1.516], loss: 8.407497, mean_absolute_error: 37.827701, mean_q: 75.313142, mean_eps: 0.050000\n",
      " 46832/50000: episode: 474, duration: 0.028s, episode steps: 11, steps per second: 391, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.113 [-2.046, 1.392], loss: 0.274229, mean_absolute_error: 37.500015, mean_q: 74.860122, mean_eps: 0.050000\n",
      " 46871/50000: episode: 475, duration: 0.096s, episode steps: 39, steps per second: 408, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.641 [0.000, 1.000], mean observation: -0.069 [-3.441, 2.350], loss: 14.170004, mean_absolute_error: 38.533807, mean_q: 76.608927, mean_eps: 0.050000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 46881/50000: episode: 476, duration: 0.027s, episode steps: 10, steps per second: 377, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.144 [-2.426, 1.539], loss: 7.218714, mean_absolute_error: 38.101210, mean_q: 76.038538, mean_eps: 0.050000\n",
      " 46891/50000: episode: 477, duration: 0.027s, episode steps: 10, steps per second: 375, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.112 [-2.964, 1.978], loss: 14.289712, mean_absolute_error: 38.612946, mean_q: 77.099623, mean_eps: 0.050000\n",
      " 46901/50000: episode: 478, duration: 0.027s, episode steps: 10, steps per second: 367, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.150 [-2.704, 1.725], loss: 13.472761, mean_absolute_error: 38.784985, mean_q: 77.337586, mean_eps: 0.050000\n",
      " 46912/50000: episode: 479, duration: 0.028s, episode steps: 11, steps per second: 396, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.125 [-3.267, 2.149], loss: 8.752402, mean_absolute_error: 38.986160, mean_q: 77.459565, mean_eps: 0.050000\n",
      " 46923/50000: episode: 480, duration: 0.028s, episode steps: 11, steps per second: 393, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.116 [-2.795, 1.777], loss: 21.182093, mean_absolute_error: 39.018772, mean_q: 77.242847, mean_eps: 0.050000\n",
      " 46932/50000: episode: 481, duration: 0.023s, episode steps: 9, steps per second: 390, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.124 [-2.763, 1.758], loss: 20.745302, mean_absolute_error: 39.268443, mean_q: 78.118038, mean_eps: 0.050000\n",
      " 46940/50000: episode: 482, duration: 0.021s, episode steps: 8, steps per second: 386, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.149 [-2.571, 1.603], loss: 19.178852, mean_absolute_error: 38.922846, mean_q: 77.305273, mean_eps: 0.050000\n",
      " 46949/50000: episode: 483, duration: 0.023s, episode steps: 9, steps per second: 391, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.129 [-2.809, 1.811], loss: 62.000381, mean_absolute_error: 39.359296, mean_q: 76.868569, mean_eps: 0.050000\n",
      " 46959/50000: episode: 484, duration: 0.027s, episode steps: 10, steps per second: 366, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.127 [-2.512, 1.538], loss: 13.668016, mean_absolute_error: 39.178487, mean_q: 77.770452, mean_eps: 0.050000\n",
      " 46968/50000: episode: 485, duration: 0.023s, episode steps: 9, steps per second: 385, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.155 [-2.858, 1.773], loss: 6.678856, mean_absolute_error: 39.013245, mean_q: 77.635590, mean_eps: 0.050000\n",
      " 46977/50000: episode: 486, duration: 0.023s, episode steps: 9, steps per second: 388, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.174 [-2.867, 1.725], loss: 17.667705, mean_absolute_error: 38.867310, mean_q: 77.325236, mean_eps: 0.050000\n",
      " 46988/50000: episode: 487, duration: 0.028s, episode steps: 11, steps per second: 391, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.119 [-2.787, 1.749], loss: 7.378503, mean_absolute_error: 38.691976, mean_q: 77.261248, mean_eps: 0.050000\n",
      " 47000/50000: episode: 488, duration: 0.033s, episode steps: 12, steps per second: 363, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.100 [-2.175, 1.408], loss: 18.619902, mean_absolute_error: 38.808924, mean_q: 77.120617, mean_eps: 0.050000\n",
      " 47009/50000: episode: 489, duration: 0.025s, episode steps: 9, steps per second: 359, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.141 [-2.807, 1.738], loss: 5.684504, mean_absolute_error: 38.611311, mean_q: 76.643513, mean_eps: 0.050000\n",
      " 47018/50000: episode: 490, duration: 0.025s, episode steps: 9, steps per second: 366, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.129 [-2.135, 1.388], loss: 30.754465, mean_absolute_error: 39.089875, mean_q: 76.805207, mean_eps: 0.050000\n",
      " 47028/50000: episode: 491, duration: 0.027s, episode steps: 10, steps per second: 369, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.136 [-1.946, 1.137], loss: 38.167409, mean_absolute_error: 39.410637, mean_q: 77.126716, mean_eps: 0.050000\n",
      " 47228/50000: episode: 492, duration: 0.484s, episode steps: 200, steps per second: 414, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.079 [-1.623, 1.696], loss: 22.210146, mean_absolute_error: 38.600035, mean_q: 76.107478, mean_eps: 0.050000\n",
      " 47237/50000: episode: 493, duration: 0.023s, episode steps: 9, steps per second: 387, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.128 [-1.384, 2.170], loss: 30.449285, mean_absolute_error: 38.685715, mean_q: 76.534686, mean_eps: 0.050000\n",
      " 47246/50000: episode: 494, duration: 0.023s, episode steps: 9, steps per second: 386, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.127 [-1.220, 1.888], loss: 27.713943, mean_absolute_error: 38.703143, mean_q: 76.483459, mean_eps: 0.050000\n",
      " 47255/50000: episode: 495, duration: 0.023s, episode steps: 9, steps per second: 388, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.134 [-1.520, 2.428], loss: 36.613490, mean_absolute_error: 38.770868, mean_q: 76.681125, mean_eps: 0.050000\n",
      " 47263/50000: episode: 496, duration: 0.021s, episode steps: 8, steps per second: 386, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.125 [0.000, 1.000], mean observation: 0.147 [-1.370, 2.234], loss: 31.068034, mean_absolute_error: 39.051658, mean_q: 77.636926, mean_eps: 0.050000\n",
      " 47272/50000: episode: 497, duration: 0.023s, episode steps: 9, steps per second: 384, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.154 [-1.335, 2.150], loss: 43.859307, mean_absolute_error: 38.775925, mean_q: 76.656062, mean_eps: 0.050000\n",
      " 47281/50000: episode: 498, duration: 0.023s, episode steps: 9, steps per second: 389, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.150 [-1.545, 2.461], loss: 8.223318, mean_absolute_error: 38.427727, mean_q: 76.730060, mean_eps: 0.050000\n",
      " 47290/50000: episode: 499, duration: 0.023s, episode steps: 9, steps per second: 392, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.143 [-1.552, 2.506], loss: 48.984950, mean_absolute_error: 38.675479, mean_q: 76.267030, mean_eps: 0.050000\n",
      " 47299/50000: episode: 500, duration: 0.023s, episode steps: 9, steps per second: 388, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.142 [-1.595, 2.463], loss: 8.559328, mean_absolute_error: 38.892815, mean_q: 77.726981, mean_eps: 0.050000\n",
      " 47309/50000: episode: 501, duration: 0.027s, episode steps: 10, steps per second: 370, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.115 [-1.995, 3.046], loss: 13.832583, mean_absolute_error: 38.695703, mean_q: 76.877847, mean_eps: 0.050000\n",
      " 47319/50000: episode: 502, duration: 0.026s, episode steps: 10, steps per second: 391, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.130 [-1.955, 3.006], loss: 24.796205, mean_absolute_error: 39.237926, mean_q: 77.961678, mean_eps: 0.050000\n",
      " 47329/50000: episode: 503, duration: 0.026s, episode steps: 10, steps per second: 388, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.129 [-1.709, 2.668], loss: 4.947871, mean_absolute_error: 38.423867, mean_q: 77.002250, mean_eps: 0.050000\n",
      " 47340/50000: episode: 504, duration: 0.028s, episode steps: 11, steps per second: 397, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.097 [-1.610, 2.396], loss: 22.231252, mean_absolute_error: 38.843284, mean_q: 77.537626, mean_eps: 0.050000\n",
      " 47350/50000: episode: 505, duration: 0.026s, episode steps: 10, steps per second: 389, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.137 [-1.774, 2.694], loss: 16.559870, mean_absolute_error: 39.260420, mean_q: 78.355232, mean_eps: 0.050000\n",
      " 47359/50000: episode: 506, duration: 0.023s, episode steps: 9, steps per second: 391, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.132 [-1.592, 2.500], loss: 4.315844, mean_absolute_error: 39.077072, mean_q: 78.182457, mean_eps: 0.050000\n",
      " 47367/50000: episode: 507, duration: 0.021s, episode steps: 8, steps per second: 386, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.125 [0.000, 1.000], mean observation: 0.160 [-1.357, 2.237], loss: 33.543052, mean_absolute_error: 38.464683, mean_q: 76.206309, mean_eps: 0.050000\n",
      " 47376/50000: episode: 508, duration: 0.024s, episode steps: 9, steps per second: 380, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.140 [-1.800, 2.817], loss: 24.509900, mean_absolute_error: 38.875237, mean_q: 77.223946, mean_eps: 0.050000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 47386/50000: episode: 509, duration: 0.026s, episode steps: 10, steps per second: 390, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.129 [-1.533, 2.515], loss: 6.023609, mean_absolute_error: 38.551815, mean_q: 77.507533, mean_eps: 0.050000\n",
      " 47396/50000: episode: 510, duration: 0.028s, episode steps: 10, steps per second: 361, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.118 [-1.925, 2.971], loss: 13.862841, mean_absolute_error: 38.977180, mean_q: 78.299212, mean_eps: 0.050000\n",
      " 47405/50000: episode: 511, duration: 0.024s, episode steps: 9, steps per second: 378, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.150 [-1.782, 2.828], loss: 1.088944, mean_absolute_error: 38.385212, mean_q: 77.082311, mean_eps: 0.050000\n",
      " 47414/50000: episode: 512, duration: 0.023s, episode steps: 9, steps per second: 387, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.132 [-1.806, 2.770], loss: 1.317899, mean_absolute_error: 39.725552, mean_q: 79.604927, mean_eps: 0.050000\n",
      " 47424/50000: episode: 513, duration: 0.026s, episode steps: 10, steps per second: 390, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.148 [-1.947, 3.041], loss: 11.947231, mean_absolute_error: 39.550937, mean_q: 79.328497, mean_eps: 0.050000\n",
      " 47434/50000: episode: 514, duration: 0.025s, episode steps: 10, steps per second: 395, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.102 [-2.002, 2.990], loss: 29.084409, mean_absolute_error: 39.173184, mean_q: 77.787357, mean_eps: 0.050000\n",
      " 47444/50000: episode: 515, duration: 0.026s, episode steps: 10, steps per second: 391, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.136 [-1.999, 3.093], loss: 12.513108, mean_absolute_error: 38.859955, mean_q: 77.710415, mean_eps: 0.050000\n",
      " 47453/50000: episode: 516, duration: 0.023s, episode steps: 9, steps per second: 389, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.127 [-1.613, 2.444], loss: 57.237857, mean_absolute_error: 39.658816, mean_q: 78.146479, mean_eps: 0.050000\n",
      " 47464/50000: episode: 517, duration: 0.028s, episode steps: 11, steps per second: 386, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.120 [-1.402, 2.152], loss: 19.171269, mean_absolute_error: 39.705397, mean_q: 78.872878, mean_eps: 0.050000\n",
      " 47474/50000: episode: 518, duration: 0.028s, episode steps: 10, steps per second: 356, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.164 [-1.337, 2.221], loss: 46.969267, mean_absolute_error: 39.167878, mean_q: 76.872850, mean_eps: 0.050000\n",
      " 47484/50000: episode: 519, duration: 0.025s, episode steps: 10, steps per second: 393, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.102 [-1.601, 2.352], loss: 32.090504, mean_absolute_error: 39.592284, mean_q: 77.920849, mean_eps: 0.050000\n",
      " 47494/50000: episode: 520, duration: 0.026s, episode steps: 10, steps per second: 392, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.115 [-1.189, 1.860], loss: 35.669827, mean_absolute_error: 39.765532, mean_q: 77.959761, mean_eps: 0.050000\n",
      " 47503/50000: episode: 521, duration: 0.024s, episode steps: 9, steps per second: 378, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.143 [-1.392, 2.197], loss: 49.374597, mean_absolute_error: 39.474340, mean_q: 77.674756, mean_eps: 0.050000\n",
      " 47514/50000: episode: 522, duration: 0.028s, episode steps: 11, steps per second: 393, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.126 [-1.733, 2.610], loss: 34.871230, mean_absolute_error: 39.616657, mean_q: 78.175740, mean_eps: 0.050000\n",
      " 47523/50000: episode: 523, duration: 0.023s, episode steps: 9, steps per second: 386, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.139 [-1.605, 2.532], loss: 20.875338, mean_absolute_error: 39.941404, mean_q: 79.161921, mean_eps: 0.050000\n",
      " 47533/50000: episode: 524, duration: 0.025s, episode steps: 10, steps per second: 393, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.118 [-1.972, 2.980], loss: 33.379025, mean_absolute_error: 39.218640, mean_q: 77.415292, mean_eps: 0.050000\n",
      " 47543/50000: episode: 525, duration: 0.027s, episode steps: 10, steps per second: 373, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.138 [-1.758, 2.749], loss: 30.526149, mean_absolute_error: 39.549155, mean_q: 78.083356, mean_eps: 0.050000\n",
      " 47553/50000: episode: 526, duration: 0.026s, episode steps: 10, steps per second: 388, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.133 [-1.374, 2.219], loss: 28.249672, mean_absolute_error: 38.921301, mean_q: 76.888972, mean_eps: 0.050000\n",
      " 47564/50000: episode: 527, duration: 0.028s, episode steps: 11, steps per second: 391, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.120 [-1.939, 2.942], loss: 26.258310, mean_absolute_error: 38.856518, mean_q: 76.640093, mean_eps: 0.050000\n",
      " 47574/50000: episode: 528, duration: 0.026s, episode steps: 10, steps per second: 386, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.135 [-1.585, 2.413], loss: 14.550342, mean_absolute_error: 38.825196, mean_q: 77.007529, mean_eps: 0.050000\n",
      " 47583/50000: episode: 529, duration: 0.023s, episode steps: 9, steps per second: 391, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.146 [-1.148, 1.902], loss: 26.343317, mean_absolute_error: 38.940822, mean_q: 76.942075, mean_eps: 0.050000\n",
      " 47592/50000: episode: 530, duration: 0.024s, episode steps: 9, steps per second: 381, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.140 [-1.128, 1.881], loss: 36.098063, mean_absolute_error: 39.346619, mean_q: 77.558547, mean_eps: 0.050000\n",
      " 47603/50000: episode: 531, duration: 0.029s, episode steps: 11, steps per second: 377, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.124 [-1.331, 2.029], loss: 34.995057, mean_absolute_error: 39.693385, mean_q: 78.433274, mean_eps: 0.050000\n",
      " 47614/50000: episode: 532, duration: 0.029s, episode steps: 11, steps per second: 383, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.128 [-1.331, 2.058], loss: 9.623728, mean_absolute_error: 39.454410, mean_q: 78.115927, mean_eps: 0.050000\n",
      " 47814/50000: episode: 533, duration: 0.507s, episode steps: 200, steps per second: 395, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.195 [-1.377, 1.374], loss: 18.254255, mean_absolute_error: 39.262830, mean_q: 77.253399, mean_eps: 0.050000\n",
      " 48014/50000: episode: 534, duration: 0.485s, episode steps: 200, steps per second: 413, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.027 [-0.596, 0.865], loss: 26.330663, mean_absolute_error: 38.760597, mean_q: 75.716031, mean_eps: 0.050000\n",
      " 48214/50000: episode: 535, duration: 0.498s, episode steps: 200, steps per second: 402, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.016 [-0.812, 0.609], loss: 16.565482, mean_absolute_error: 38.310415, mean_q: 75.262891, mean_eps: 0.050000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 48414/50000: episode: 536, duration: 0.513s, episode steps: 200, steps per second: 390, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.001 [-0.485, 0.532], loss: 18.542920, mean_absolute_error: 38.098516, mean_q: 74.820585, mean_eps: 0.050000\n",
      " 48614/50000: episode: 537, duration: 0.510s, episode steps: 200, steps per second: 392, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.023 [-0.827, 0.560], loss: 17.372756, mean_absolute_error: 37.852730, mean_q: 74.416143, mean_eps: 0.050000\n",
      " 48814/50000: episode: 538, duration: 0.550s, episode steps: 200, steps per second: 364, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.026 [-0.417, 0.588], loss: 22.856058, mean_absolute_error: 37.702851, mean_q: 73.926269, mean_eps: 0.050000\n",
      " 49014/50000: episode: 539, duration: 0.519s, episode steps: 200, steps per second: 386, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: -0.005 [-0.555, 0.539], loss: 15.018917, mean_absolute_error: 37.516565, mean_q: 73.777101, mean_eps: 0.050000\n",
      " 49214/50000: episode: 540, duration: 0.511s, episode steps: 200, steps per second: 392, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.019 [-0.320, 0.403], loss: 17.499670, mean_absolute_error: 36.831252, mean_q: 72.395035, mean_eps: 0.050000\n",
      " 49414/50000: episode: 541, duration: 0.503s, episode steps: 200, steps per second: 398, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.015 [-0.541, 0.592], loss: 17.544911, mean_absolute_error: 36.590560, mean_q: 71.880004, mean_eps: 0.050000\n",
      " 49614/50000: episode: 542, duration: 0.505s, episode steps: 200, steps per second: 396, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.039 [-0.534, 0.434], loss: 15.683081, mean_absolute_error: 35.867037, mean_q: 70.635595, mean_eps: 0.050000\n",
      " 49814/50000: episode: 543, duration: 0.479s, episode steps: 200, steps per second: 417, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.003 [-0.523, 0.636], loss: 14.392211, mean_absolute_error: 35.628826, mean_q: 70.183629, mean_eps: 0.050000\n",
      "done, took 134.642 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x10c6a3d30>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.fit(env, nb_steps=50000, visualize=True, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 10 episodes ...\n",
      "Episode 1: reward: 200.000, steps: 200\n",
      "Episode 2: reward: 200.000, steps: 200\n",
      "Episode 3: reward: 200.000, steps: 200\n",
      "Episode 4: reward: 200.000, steps: 200\n",
      "Episode 5: reward: 200.000, steps: 200\n",
      "Episode 6: reward: 200.000, steps: 200\n",
      "Episode 7: reward: 200.000, steps: 200\n",
      "Episode 8: reward: 200.000, steps: 200\n",
      "Episode 9: reward: 200.000, steps: 200\n",
      "Episode 10: reward: 200.000, steps: 200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x10c6a3e80>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.test(env, nb_episodes=10, visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
